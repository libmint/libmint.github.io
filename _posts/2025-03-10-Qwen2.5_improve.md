ì£¼ì‹  ë°ì´í„° í˜•ì‹ì— ë§ê²Œ Qwen2.5-VL ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” FastAPI ì„œë²„ ì½”ë“œë¥¼ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

## âœ… ìˆ˜ì •ëœ ì½”ë“œ (Qwen2.5-VL ëª¨ë¸ ê¸°ë°˜)

```python
import os
import re
import json
import torch
import pickle
import argparse
from fastapi import FastAPI
from pydantic import BaseModel
import base64
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import uvicorn
from fastapi.middleware.cors import CORSMiddleware

# ì¸ì ì„¤ì •
parser = argparse.ArgumentParser()
parser.add_argument("--device", type=str, default="cuda:0")
parser.add_argument("--model_name_or_path", type=str,
                    default="Qwen/Qwen2.5-VL-7B-Instruct")
parser.add_argument("--port", type=int, default=8080)
args = parser.parse_args()

print("Loaded model:", args.model_name_or_path)

# í”„ë¡œì„¸ì„œ ì„¤ì • (ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
processor = AutoProcessor.from_pretrained(args.model_name_or_path)

# "" í† í° ì¶”ê°€ (í•„ìš”í•œ ê²½ìš°)
if "" not in processor.tokenizer.additional_special_tokens:
    processor.tokenizer.add_special_tokens({"additional_special_tokens": [""]})
    processor.save_pretrained(args.model_name_or_path)

image_token_id = processor.tokenizer.convert_tokens_to_ids("")

# ë°ì´í„° ì½œë ˆì´í„° ì •ì˜ (ì£¼ì–´ì§„ ë°ì´í„° í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •)
class MyDataCollator:
    def __init__(self, processor, image_token_id):
        self.processor = processor
        self.image_token_id = image_token_id

    def __call__(self, example_list):
        texts, images = [], []
        for example in example_list:
            image_list = example["images"]
            conversations = example["conversations"]

            # messages í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (Qwen2.5-VLì˜ chat_templateì— ë§ê²Œ)
            messages = []
            image_idx = 0  # ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ê´€ë¦¬ìš©

            for conv in conversations:
                role = "user" if conv["from"] == "human" else "assistant"
                content_raw = conv["value"]

                # "" í† í° ì²˜ë¦¬ ë° ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ì¦ê°€
                segments = re.split(r'()', content_raw)
                content_processed = []
                for seg in segments:
                    if seg == "":
                        content_processed.append({"type": "image"})
                        image_idx += 1
                    elif seg.strip():
                        content_processed.append({"type": "text", "text": seg})

                messages.append({"role": role, "content": content_processed})

            # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
            text_input = processor.apply_chat_template(messages, add_generation_prompt=True)
            texts.append(text_input.strip())
            images.append(image_list)

        batch = processor(text=texts, images=images, return_tensors="pt", padding=True)
        return batch

# ëª¨ë¸ ë¡œë”©
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    args.model_name_or_path,
    torch_dtype=torch.float16,
).to(args.device)

data_collator = MyDataCollator(processor, image_token_id)

# FastAPI ë°ì´í„° ëª¨ë¸ ì •ì˜ (ì…ë ¥ í˜•ì‹ì— ë§ê²Œ)
class Conversation(BaseModel):
    from_: str  # 'from'ì€ ì˜ˆì•½ì–´ë¼ from_ìœ¼ë¡œ ì‚¬ìš© í›„ aliasë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë‚˜ ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ from_ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    value: str

class InputData(BaseModel):
    id: str
    conversations: list[Conversation]
    images: str  # base64ë¡œ ì¸ì½”ë”©ëœ pickle.dumps(image_list)ì˜ ê²°ê³¼ë¬¼

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/predict")
def predict(example: InputData):
    example_dict = example.dict()

    # ì´ë¯¸ì§€ ë””ì½”ë”© ë° ë¡œë“œ
    image_list_bin = base64.b64decode(example_dict["images"])
    image_list_pil = pickle.loads(image_list_bin)  # PIL.Image ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¡œë“œë¨

    # ë°ì´í„° í˜•ì‹ ë³€í™˜ (ì£¼ì–´ì§„ í˜•ì‹ì— ë§ê²Œ)
    converted_example = {
        "id": example_dict["id"],
        "images": image_list_pil,
        "conversations": [{"from": conv["from_"], "value": conv["value"]} for conv in example_dict["conversations"]]
    }

    batch = data_collator([converted_example])
    batch = {k: v.to(args.device) for k, v in batch.items()}

    with torch.no_grad():
        generated_ids = model.generate(
            **batch,
            max_new_tokens=256,
            min_new_tokens=3,
            eos_token_id=processor.tokenizer.eos_token_id,
            do_sample=True,
            temperature=1.2,
        )

    generated_text = processor.batch_decode(
        generated_ids[:, batch["input_ids"].size(1):],
        skip_special_tokens=True
    )[0]

    input_token_count = batch["input_ids"].size(1)
    output_token_count = generated_ids.size(1) - input_token_count

    return {
        "text": generated_text,
        "prompt_tokens": input_token_count,
        "completion_tokens": output_token_count
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=args.port)
```

---

## ğŸ“Œ ì£¼ìš” ìˆ˜ì • ì‚¬í•­ ë° íŠ¹ì§•:

- **ë°ì´í„° í˜•ì‹**  
  ì£¼ì–´ì§„ JSON ë°ì´í„° í˜•ì‹ì„ ì •í™•íˆ ë”°ë¥´ë„ë¡ ë°ì´í„°ë¥¼ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.
  
- **íŠ¹ìˆ˜ í† í° `` ì²˜ë¦¬**  
  `` í† í°ì„ tokenizerì— ì¶”ê°€í•˜ê³  IDë¥¼ ì–»ì–´ ì‚¬ìš©í•©ë‹ˆë‹¤.
  
- **ì´ë¯¸ì§€ ì²˜ë¦¬ ë°©ì‹**  
  ì…ë ¥ ì´ë¯¸ì§€ëŠ” base64ë¡œ ì¸ì½”ë”©ëœ pickle ê°ì²´ë¡œ ë°›ì•„ì„œ PIL ì´ë¯¸ì§€ í˜•íƒœë¡œ ë³µì›í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.

- **FastAPI ì…ë ¥ ëª¨ë¸ ì •ì˜**  
  Pydantic ëª¨ë¸ì„ ëª…í™•íˆ ì •ì˜í•˜ì—¬ API ì¸í„°í˜ì´ìŠ¤ë¥¼ ëª…í™•íˆ í–ˆìŠµë‹ˆë‹¤.

- **ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë”©**  
  Qwen2.5-VL ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ ì •í™•íˆ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.

---

## ğŸš© í…ŒìŠ¤íŠ¸ ë°©ë²• ì˜ˆì‹œ:

ë‹¤ìŒê³¼ ê°™ì´ ìš”ì²­ì„ ë³´ë‚´ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import requests, pickle, base64
from PIL import Image

url = "http://localhost:8080/predict"

# ì˜ˆì‹œ ì´ë¯¸ì§€ ë¡œë“œ ë° ì¸ì½”ë”© (ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ì ì ˆíˆ ë°”ê¿”ì£¼ì„¸ìš”.)
img_paths = ["img1.png", "img2.png"]
pil_images = [Image.open(p).convert("RGB") for p in img_paths]
encoded_images_str = base64.b64encode(pickle.dumps(pil_images)).decode("utf-8")

example_payload = {
  "id": "test123",
  "images": encoded_images_str,
  "conversations": [
      {"from_": "human", "value": " ì´ ê·¸ë¦¼ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?"},
      {"from_": "gpt", "value": "Thought: ì´ ê·¸ë¦¼ì€ ë™ë¬¼ ì‚¬ì§„ì…ë‹ˆë‹¤.\n\nAction: ê°œê°€ ë³´ì…ë‹ˆë‹¤."}
  ]
}

response = requests.post(url, json=example_payload)
print(response.json())
```

```
import requests

url = "http://localhost:8080/predict"

example_payload = {
  "id": "test123",
  "images": ["./img1.png", "./img2.png"],  # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ì§ì ‘ ì…ë ¥í•©ë‹ˆë‹¤.
  "conversations": [
      {"from_": "human", "value": "<image> ì´ ê·¸ë¦¼ì—ì„œ ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?"},
      {"from_": "gpt", "value": "Thought: ì´ ê·¸ë¦¼ì€ ë™ë¬¼ ì‚¬ì§„ì…ë‹ˆë‹¤.\n\nAction: ê°œê°€ ë³´ì…ë‹ˆë‹¤."}
  ]
}

response = requests.post(url, json=example_payload)
print(response.json())
```
