주신 데이터 형식에 맞게 Qwen2.5-VL 모델을 사용하는 FastAPI 서버 코드를 수정했습니다.

## ✅ 수정된 코드 (Qwen2.5-VL 모델 기반)

```python
import os
import re
import json
import torch
import pickle
import argparse
from fastapi import FastAPI
from pydantic import BaseModel
import base64
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import uvicorn
from fastapi.middleware.cors import CORSMiddleware

# 인자 설정
parser = argparse.ArgumentParser()
parser.add_argument("--device", type=str, default="cuda:0")
parser.add_argument("--model_name_or_path", type=str,
                    default="Qwen/Qwen2.5-VL-7B-Instruct")
parser.add_argument("--port", type=int, default=8080)
args = parser.parse_args()

print("Loaded model:", args.model_name_or_path)

# 프로세서 설정 (기본 설정 사용)
processor = AutoProcessor.from_pretrained(args.model_name_or_path)

# "" 토큰 추가 (필요한 경우)
if "" not in processor.tokenizer.additional_special_tokens:
    processor.tokenizer.add_special_tokens({"additional_special_tokens": [""]})
    processor.save_pretrained(args.model_name_or_path)

image_token_id = processor.tokenizer.convert_tokens_to_ids("")

# 데이터 콜레이터 정의 (주어진 데이터 형식에 맞게 수정)
class MyDataCollator:
    def __init__(self, processor, image_token_id):
        self.processor = processor
        self.image_token_id = image_token_id

    def __call__(self, example_list):
        texts, images = [], []
        for example in example_list:
            image_list = example["images"]
            conversations = example["conversations"]

            # messages 형식으로 변환 (Qwen2.5-VL의 chat_template에 맞게)
            messages = []
            image_idx = 0  # 이미지 인덱스 관리용

            for conv in conversations:
                role = "user" if conv["from"] == "human" else "assistant"
                content_raw = conv["value"]

                # "" 토큰 처리 및 이미지 인덱스 증가
                segments = re.split(r'()', content_raw)
                content_processed = []
                for seg in segments:
                    if seg == "":
                        content_processed.append({"type": "image"})
                        image_idx += 1
                    elif seg.strip():
                        content_processed.append({"type": "text", "text": seg})

                messages.append({"role": role, "content": content_processed})

            # 텍스트와 이미지 리스트 준비
            text_input = processor.apply_chat_template(messages, add_generation_prompt=True)
            texts.append(text_input.strip())
            images.append(image_list)

        batch = processor(text=texts, images=images, return_tensors="pt", padding=True)
        return batch

# 모델 로딩
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    args.model_name_or_path,
    torch_dtype=torch.float16,
).to(args.device)

data_collator = MyDataCollator(processor, image_token_id)

# FastAPI 데이터 모델 정의 (입력 형식에 맞게)
class Conversation(BaseModel):
    from_: str  # 'from'은 예약어라 from_으로 사용 후 alias로 처리 가능하나 여기서는 단순히 from_으로 사용합니다.
    value: str

class InputData(BaseModel):
    id: str
    conversations: list[Conversation]
    images: str  # base64로 인코딩된 pickle.dumps(image_list)의 결과물

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/predict")
def predict(example: InputData):
    example_dict = example.dict()

    # 이미지 디코딩 및 로드
    image_list_bin = base64.b64decode(example_dict["images"])
    image_list_pil = pickle.loads(image_list_bin)  # PIL.Image 객체 리스트로 로드됨

    # 데이터 형식 변환 (주어진 형식에 맞게)
    converted_example = {
        "id": example_dict["id"],
        "images": image_list_pil,
        "conversations": [{"from": conv["from_"], "value": conv["value"]} for conv in example_dict["conversations"]]
    }

    batch = data_collator([converted_example])
    batch = {k: v.to(args.device) for k, v in batch.items()}

    with torch.no_grad():
        generated_ids = model.generate(
            **batch,
            max_new_tokens=256,
            min_new_tokens=3,
            eos_token_id=processor.tokenizer.eos_token_id,
            do_sample=True,
            temperature=1.2,
        )

    generated_text = processor.batch_decode(
        generated_ids[:, batch["input_ids"].size(1):],
        skip_special_tokens=True
    )[0]

    input_token_count = batch["input_ids"].size(1)
    output_token_count = generated_ids.size(1) - input_token_count

    return {
        "text": generated_text,
        "prompt_tokens": input_token_count,
        "completion_tokens": output_token_count
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=args.port)
```

---

## 📌 주요 수정 사항 및 특징:

- **데이터 형식**  
  주어진 JSON 데이터 형식을 정확히 따르도록 데이터를 처리했습니다.
  
- **특수 토큰 `` 처리**  
  `` 토큰을 tokenizer에 추가하고 ID를 얻어 사용합니다.
  
- **이미지 처리 방식**  
  입력 이미지는 base64로 인코딩된 pickle 객체로 받아서 PIL 이미지 형태로 복원하여 사용합니다.

- **FastAPI 입력 모델 정의**  
  Pydantic 모델을 명확히 정의하여 API 인터페이스를 명확히 했습니다.

- **모델 및 프로세서 로딩**  
  Qwen2.5-VL 모델과 프로세서를 transformers 라이브러리를 통해 정확히 로드했습니다.

---

## 🚩 테스트 방법 예시:

다음과 같이 요청을 보내 테스트할 수 있습니다.

```python
import requests, pickle, base64
from PIL import Image

url = "http://localhost:8080/predict"

# 예시 이미지 로드 및 인코딩 (이미지 파일 경로를 적절히 바꿔주세요.)
img_paths = ["img1.png", "img2.png"]
pil_images = [Image.open(p).convert("RGB") for p in img_paths]
encoded_images_str = base64.b64encode(pickle.dumps(pil_images)).decode("utf-8")

example_payload = {
  "id": "test123",
  "images": encoded_images_str,
  "conversations": [
      {"from_": "human", "value": " 이 그림에서 무엇이 보이나요?"},
      {"from_": "gpt", "value": "Thought: 이 그림은 동물 사진입니다.\n\nAction: 개가 보입니다."}
  ]
}

response = requests.post(url, json=example_payload)
print(response.json())
```

```
import os
import re
import torch
import argparse
from fastapi import FastAPI
from pydantic import BaseModel
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import uvicorn
from fastapi.middleware.cors import CORSMiddleware

# 인자 설정
parser = argparse.ArgumentParser()
parser.add_argument("--device", type=str, default="cuda:0")
parser.add_argument("--model_name_or_path", type=str,
                    default="Qwen/Qwen2.5-VL-7B-Instruct")
parser.add_argument("--port", type=int, default=8080)
args = parser.parse_args()

print("Loaded model:", args.model_name_or_path)

# 프로세서 설정 (기본 설정 사용)
processor = AutoProcessor.from_pretrained(args.model_name_or_path)

# "<image>" 토큰 추가 (필요한 경우)
if "<image>" not in processor.tokenizer.additional_special_tokens:
    processor.tokenizer.add_special_tokens({"additional_special_tokens": ["<image>"]})
    processor.save_pretrained(args.model_name_or_path)

image_token_id = processor.tokenizer.convert_tokens_to_ids("<image>")

# 데이터 콜레이터 정의 (주어진 데이터 형식에 맞게 수정)
class MyDataCollator:
    def __init__(self, processor, image_token_id):
        self.processor = processor
        self.image_token_id = image_token_id

    def __call__(self, example_list):
        texts, images = [], []
        for example in example_list:
            image_paths = example["images"]
            conversations = example["conversations"]

            # 이미지 파일 경로에서 PIL 이미지 로드
            image_list_pil = [Image.open(path).convert("RGB") for path in image_paths]

            # messages 형식 변환 (Qwen2.5-VL의 chat_template에 맞게)
            messages = []
            for conv in conversations:
                role = "user" if conv["from"] == "human" else "assistant"
                content_raw = conv["value"]

                segments = re.split(r'(<image>)', content_raw)
                content_processed = []
                for seg in segments:
                    if seg == "<image>":
                        content_processed.append({"type": "image"})
                    elif seg.strip():
                        content_processed.append({"type": "text", "text": seg})

                messages.append({"role": role, "content": content_processed})

            text_input = processor.apply_chat_template(messages, add_generation_prompt=True)
            texts.append(text_input.strip())
            images.append(image_list_pil)

        batch = processor(text=texts, images=images, return_tensors="pt", padding=True)
        return batch

# 모델 로딩
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    args.model_name_or_path,
    torch_dtype=torch.float16,
).to(args.device)

data_collator = MyDataCollator(processor, image_token_id)

# FastAPI 입력 모델 정의 (이미지 경로를 직접 받도록 수정)
class Conversation(BaseModel):
    from_: str  # 'from' 예약어 방지용 from_
    value: str

class InputData(BaseModel):
    id: str
    conversations: list[Conversation]
    images: list[str]  # 이미지 파일 경로 리스트로 받음

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/predict")
def predict(example: InputData):
    example_dict = example.dict()

    converted_example = {
        "id": example_dict["id"],
        "images": example_dict["images"],  # 이미지 경로 그대로 전달
        "conversations": [{"from": conv["from_"], "value": conv["value"]} for conv in example_dict["conversations"]]
    }

    batch = data_collator([converted_example])
    batch = {k: v.to(args.device) for k, v in batch.items()}

    with torch.no_grad():
        generated_ids = model.generate(
            **batch,
            max_new_tokens=256,
            min_new_tokens=3,
            eos_token_id=processor.tokenizer.eos_token_id,
            do_sample=True,
            temperature=1.2,
        )

    generated_text = processor.batch_decode(
        generated_ids[:, batch["input_ids"].size(1):],
        skip_special_tokens=True
    )[0]

    input_token_count = batch["input_ids"].size(1)
    output_token_count = generated_ids.size(1) - input_token_count

    return {
        "text": generated_text,
        "prompt_tokens": input_token_count,
        "completion_tokens": output_token_count
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=args.port)

```
