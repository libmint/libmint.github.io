```
import argparse
import os
import sys
import torch
from PIL import Image

# [수정X] 원본과 동일한 임포트 경로 유지
from wan.configs import WAN_CONFIGS
from wan.utils.utils import cache_video, str2bool
from wan.modules.model import WanModel

def generate():
    parser = argparse.ArgumentParser()
    # [수정X] 원본 인자 구성 그대로 유지
    parser.add_argument("--task", type=str, default="i2v-A14B", choices=list(WAN_CONFIGS.keys()))
    parser.add_argument("--size", type=str, default="480*832", choices=["480*832", "720*1280"])
    parser.add_argument("--ckpt_dir", type=str, required=True)
    parser.add_argument("--image", type=str, required=True)
    parser.add_argument("--prompt", type=str, default=None)
    parser.add_argument("--ulysses_size", type=int, default=1)
    parser.add_argument("--ring_size", type=int, default=1)
    parser.add_argument("--t5_cpu", type=str2bool, default=True)
    parser.add_argument("--dit_fsdp", action="store_true", default=False)
    parser.add_argument("--t5_fsdp", action="store_true", default=False)
    parser.add_argument("--frame_num", type=int, default=81)
    
    args = parser.parse_args()

    # [핵심 수정] 64GB RAM 폭증을 막기 위한 로딩 파라미터 주입
    print(f"[*] 모델 로딩 시작: {args.ckpt_dir}")
    model = WanModel.from_pretrained(
        args.ckpt_dir,
        low_cpu_mem_usage=True,  # RAM에 한꺼번에 올리지 않음 (64GB OOM 방지)
        device_map="auto",       # 3090 VRAM 24GB를 효율적으로 배분
        torch_dtype=torch.bfloat16
    )

    if args.t5_cpu:
        print("[*] T5 인코더를 CPU로 이동합니다.")
        model.text_encoder.to("cpu")
        torch.cuda.empty_cache()

    # [수정X] 원본 생성 로직 유지
    img = Image.open(args.image).convert("RGB")
    w, h = map(int, args.size.split("*"))
    
    print(f"[*] 비디오 생성 시작...")
    with torch.no_grad():
        video = model.generate(
            input_image=img,
            prompt=args.prompt,
            size=(w, h),
            frame_num=args.frame_num,
            sampling_steps=40
        )

    cache_video(video, "output.mp4", fps=16)
    print("[*] 완료: output.mp4")

if __name__ == "__main__":
    generate()
```
