```
import argparse
import gc
import logging
import os
import random
import sys
from pathlib import Path
import numpy as np
import torch
import torchvision.transforms.functional as TF
from PIL import Image
from tqdm import tqdm

# 원본 구조 유지
sys.path.insert(0, str(Path(__file__).parent))
from einops import rearrange
from load_prequant import load_quantized_model
from wan.configs.wan_i2v_A14B import i2v_A14B as cfg
from wan.modules.t5 import T5EncoderModel
from wan.modules.vae2_1 import Wan2_1_VAE
from wan.utils.cam_utils import (
    compute_relative_poses,
    get_Ks_transformed,
    get_plucker_embeddings,
    interpolate_camera_poses,
)
from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WanI2V_PreQuant:
    """RTX 3090 & 64GB RAM 최적화 버전"""
    def __init__(
        self,
        checkpoint_dir: str,
        device_id: int = 0,
        t5_cpu: bool = True,
    ):
        self.device = torch.device(f"cuda:{device_id}")
        self.config = cfg
        self.t5_cpu = t5_cpu
        self.num_train_timesteps = cfg.num_train_timesteps
        self.boundary = cfg.boundary
        self.param_dtype = cfg.param_dtype
        self.vae_stride = cfg.vae_stride
        self.patch_size = cfg.patch_size
        self.sample_neg_prompt = cfg.sample_neg_prompt
        
        # 가중치 경로 저장 (나중에 로드하기 위함)
        self.low_noise_dir = os.path.join(checkpoint_dir, cfg.low_noise_checkpoint + "_bnb_nf4")
        self.high_noise_dir = os.path.join(checkpoint_dir, cfg.high_noise_checkpoint + "_bnb_nf4")

        # 1. T5 로딩 (정밀도를 BF16으로 낮춰 RAM 절약)
        logger.info("Loading T5 encoder (BF16)...")
        local_tokenizer = os.path.join(checkpoint_dir, "tokenizer")
        tokenizer_path = local_tokenizer if os.path.isdir(local_tokenizer) else cfg.t5_tokenizer
        self.text_encoder = T5EncoderModel(
            text_len=cfg.text_len,
            dtype=torch.bfloat16, 
            device=torch.device("cpu"),
            checkpoint_path=os.path.join(checkpoint_dir, cfg.t5_checkpoint),
            tokenizer_path=tokenizer_path,
            shard_fn=None,
        )

        # 2. VAE 로딩
        logger.info("Loading VAE...")
        self.vae = Wan2_1_VAE(
            vae_pth=os.path.join(checkpoint_dir, cfg.vae_checkpoint),
            device=self.device,
        )

        # 3. 모델 자리만 확보 (실제 로딩은 generate 단계에서 수행)
        self.low_noise_model = None
        self.high_noise_model = None
        logger.info("Initialization complete. Models will be loaded sequentially to save RAM.")

    def _prepare_model_for_timestep(self, t, boundary):
        """현재 타임스텝에 필요한 모델만 유지하고 나머지는 메모리에서 삭제"""
        is_high = t.item() >= boundary
        target_dir = self.high_noise_dir if is_high else self.low_noise_dir
        target_attr = "high_noise_model" if is_high else "low_noise_model"
        offload_attr = "low_noise_model" if is_high else "high_noise_model"

        # 사용하지 않는 모델은 RAM/VRAM에서 완전히 삭제하여 공간 확보
        if getattr(self, offload_attr) is not None:
            logger.info(f"Offloading {offload_attr} from memory...")
            setattr(self, offload_attr, None)
            gc.collect()
            torch.cuda.empty_cache()

        # 필요한 모델이 없으면 그 때 로드
        if getattr(self, target_attr) is None:
            logger.info(f"Loading {target_attr} from disk...")
            # load_quantized_model 시점에 RAM 피크가 발생하므로 gc를 먼저 호출
            gc.collect() 
            model = load_quantized_model(target_dir, device="cpu")
            model.to(self.device)
            setattr(self, target_attr, model)

        return getattr(self, target_attr)

    def generate(
        self,
        input_prompt: str,
        img: Image.Image,
        action_path: str = None,
        max_area: int = 480 * 832,
        frame_num: int = 81,
        shift: float = 5.0,
        sampling_steps: int = 40,
        guide_scale: float = 5.0,
        n_prompt: str = "",
        seed: int = -1,
    ):
        # [기본 설정 로직]
        if action_path is not None:
            c2ws = np.load(os.path.join(action_path, "poses.npy"))
            len_c2ws = ((len(c2ws) - 1) // 4) * 4 + 1
            frame_num = min(frame_num, len_c2ws)
            c2ws = c2ws[:frame_num]
        
        guide_scale = (guide_scale, guide_scale) if isinstance(guide_scale, float) else guide_scale
        img_tensor = TF.to_tensor(img).sub_(0.5).div_(0.5).to(self.device)
        F = frame_num
        h, w = img_tensor.shape[1:]
        aspect_ratio = h / w
        lat_h = round(np.sqrt(max_area * aspect_ratio) // self.vae_stride[1] // self.patch_size[1] * self.patch_size[1])
        lat_w = round(np.sqrt(max_area / aspect_ratio) // self.vae_stride[2] // self.patch_size[2] * self.patch_size[2])
        h, w = lat_h * self.vae_stride[1], lat_w * self.vae_stride[2]
        lat_f = (F - 1) // self.vae_stride[0] + 1
        max_seq_len = lat_f * lat_h * lat_w // (self.patch_size[1] * self.patch_size[2])
        
        seed = seed if seed >= 0 else random.randint(0, sys.maxsize)
        seed_g = torch.Generator(device=self.device).manual_seed(seed)
        noise = torch.randn(16, lat_f, lat_h, lat_w, dtype=torch.float32, generator=seed_g, device=self.device)
        
        # [텍스트 인코딩]
        if n_prompt == "": n_prompt = self.sample_neg_prompt
        logger.info("Encoding prompts...")
        context = self.text_encoder([input_prompt], torch.device("cpu"))
        context_null = self.text_encoder([n_prompt], torch.device("cpu"))
        context = [t.to(self.device) for t in context]
        context_null = [t.to(self.device) for t in context_null]

        # [카메라 준비 로직 (원본 동일)]
        dit_cond_dict = None
        if action_path is not None:
            Ks = torch.from_numpy(np.load(os.path.join(action_path, "intrinsics.npy"))).float()
            Ks = get_Ks_transformed(Ks, 480, 832, h, w, h, w)[0]
            c2ws_infer = interpolate_camera_poses(np.linspace(0, len(c2ws)-1, len(c2ws)), c2ws[:, :3, :3], c2ws[:, :3, 3], np.linspace(0, len(c2ws)-1, int((len(c2ws)-1)//4)+1))
            c2ws_infer = compute_relative_poses(c2ws_infer, framewise=True).to(self.device)
            Ks = Ks.repeat(len(c2ws_infer), 1).to(self.device)
            c2ws_plucker_emb = get_plucker_embeddings(c2ws_infer, Ks, h, w)
            c2ws_plucker_emb = rearrange(c2ws_plucker_emb, "f (h c1) (w c2) c -> (f h w) (c c1 c2)", c1=int(h//lat_h), c2=int(w//lat_w))[None, ...]
            c2ws_plucker_emb = rearrange(c2ws_plucker_emb, "b (f h w) c -> b c f h w", f=lat_f, h=lat_h, w=lat_w).to(self.param_dtype)
            dit_cond_dict = {"c2ws_plucker_emb": c2ws_plucker_emb.chunk(1, dim=0)}

        # [VAE 인코딩]
        y = self.vae.encode([torch.concat([torch.nn.functional.interpolate(img_tensor[None].cpu(), size=(h, w), mode="bicubic").transpose(0, 1), torch.zeros(3, F - 1, h, w)], dim=1).to(self.device)])[0]
        msk = torch.ones(1, F, lat_h, lat_w, device=self.device); msk[:, 1:] = 0
        msk = torch.concat([torch.repeat_interleave(msk[:, 0:1], repeats=4, dim=1), msk[:, 1:]], dim=1).view(1, -1, 4, lat_h, lat_w).transpose(1, 2)[0]
        y = torch.concat([msk, y])

        # [디퓨전 샘플링]
        with torch.amp.autocast("cuda", dtype=self.param_dtype), torch.no_grad():
            boundary = self.boundary * self.num_train_timesteps
            sample_scheduler = FlowUniPCMultistepScheduler(num_train_timesteps=self.num_train_timesteps, shift=1, use_dynamic_shifting=False)
            sample_scheduler.set_timesteps(sampling_steps, device=self.device, shift=shift)
            
            latent = noise
            arg_c = {"context": [context[0]], "seq_len": max_seq_len, "y": [y], "dit_cond_dict": dit_cond_dict}
            arg_null = {"context": context_null, "seq_len": max_seq_len, "y": [y], "dit_cond_dict": dit_cond_dict}

            for _, t in enumerate(tqdm(sample_scheduler.timesteps, desc="Sampling")):
                model = self._prepare_model_for_timestep(t, boundary)
                
                noise_pred_cond = model([latent.to(self.device)], t=torch.stack([t]).to(self.device), **arg_c)[0]
                noise_pred_uncond = model([latent.to(self.device)], t=torch.stack([t]).to(self.device), **arg_null)[0]
                
                sample_guide_scale = guide_scale[1] if t.item() >= boundary else guide_scale[0]
                noise_pred = noise_pred_uncond + sample_guide_scale * (noise_pred_cond - noise_pred_uncond)
                
                latent = sample_scheduler.step(noise_pred.unsqueeze(0), t, latent.unsqueeze(0), return_dict=False, generator=seed_g)[0].squeeze(0)
                torch.cuda.empty_cache()

            # 완료 후 모델 정리
            self.low_noise_model = self.high_noise_model = None
            gc.collect(); torch.cuda.empty_cache()
            video = self.vae.decode([latent])
            
        return video[0]

def save_video(frames: torch.Tensor, output_path: str, fps: int = 16):
    import imageio
    frames = ((frames + 1) / 2 * 255).clamp(0, 255).byte()
    frames = frames.permute(1, 2, 3, 0).cpu().numpy()
    imageio.mimwrite(output_path, frames, fps=fps, codec="libx264")
    logger.info(f"Saved video to {output_path}")

def main():
    parser = argparse.ArgumentParser()
    script_dir = str(Path(__file__).parent)
    parser.add_argument("--ckpt_dir", type=str, default=script_dir)
    parser.add_argument("--image", type=str, required=True)
    parser.add_argument("--prompt", type=str, required=True)
    parser.add_argument("--size", type=str, default="480*832")
    parser.add_argument("--frame_num", type=int, default=81)
    parser.add_argument("--output", type=str, default="output.mp4")
    parser.add_argument("--t5_cpu", action="store_true", default=True)
    args = parser.parse_args()

    img = Image.open(args.image).convert("RGB")
    pipeline = WanI2V_PreQuant(checkpoint_dir=args.ckpt_dir, t5_cpu=args.t5_cpu)
    
    video = pipeline.generate(
        input_prompt=args.prompt, 
        img=img, 
        max_area=int(args.size.split('*')[0])*int(args.size.split('*')[1]),
        frame_num=args.frame_num
    )
    save_video(video, args.output)

if __name__ == "__main__":
    main()
```
