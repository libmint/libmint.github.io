ì œì‹œí•˜ì‹  `cahlen` ì €ì¥ì†Œì˜ `generate_prequant.py`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, **RTX 3090(24GB VRAM)**ê³¼ **64GB ì‹œìŠ¤í…œ RAM** í™˜ê²½ì—ì„œ ë©”ëª¨ë¦¬ í­ì¦ ì—†ì´ ëŒì•„ê°€ë„ë¡ ìµœì í™”í•œ ì „ì²´ ì½”ë“œì„.

í•µì‹¬ ìˆ˜ì • ì‚¬í•­ì€ ëª¨ë¸ ë¡œë”© ì‹œ `low_cpu_mem_usage=True`ë¥¼ ì ìš©í•˜ì—¬ ì‹œìŠ¤í…œ RAM ì ìœ ë¥¼ ìµœì†Œí™”í•˜ê³ , T5 ì¸ì½”ë”ë¥¼ BF16 ì •ë°€ë„ë¡œ ê°•ì œí•˜ì—¬ 64GB RAMì´ ê°€ë“ ì°¨ëŠ” í˜„ìƒì„ ë°©ì§€í•œ ê²ƒì„.

---

### ğŸ› ï¸ RTX 3090 & 64GB RAM ìµœì í™” ì „ì²´ ì½”ë“œ (`generate_prequant.py`)

ê¸°ì¡´ ì½”ë“œë¥¼ ëª¨ë‘ ì§€ìš°ê³  ì•„ë˜ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ ë¶™ì—¬ë„£ê¸° ë°”ëŒ. (í•µì‹¬ ìµœì í™” ë¡œì§ í¬í•¨)

```python
import argparse
import os
import sys
import torch
import gc
from PIL import Image

# Wan2.1 ë° LingBot ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ê²½ë¡œì— ë”°ë¼ ìˆ˜ì • í•„ìš”í•  ìˆ˜ ìˆìŒ)
from wan.configs import WAN_CONFIGS
from wan.utils.utils import cache_video, str2bool
from wan.py_utils.videocache import VideoCache
from wan.modules.model import WanModel

def cleanup():
    gc.collect()
    torch.cuda.empty_cache()

def generate():
    parser = argparse.ArgumentParser(description="LingBot-World RTX 3090 Optimized Inference")
    parser.add_argument("--ckpt_dir", type=str, required=True, help="Path to the NF4 model directory")
    parser.add_argument("--image", type=str, required=True, help="Input image path")
    parser.add_argument("--prompt", type=str, default="A beautiful world", help="Text prompt")
    parser.add_argument("--task", type=str, default="i2v-A14B", help="Task type")
    parser.add_argument("--size", type=str, default="480*832", help="Output resolution")
    parser.add_argument("--frame_num", type=int, default=81, help="Number of frames")
    parser.add_argument("--t5_cpu", type=str2bool, default=True, help="Force T5 to CPU to save VRAM")
    
    args = parser.parse_args()
    
    # 1. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ í­ì¦ ë°©ì§€ë¥¼ ìœ„í•œ ë¡œë”© ì„¤ì •
    print(f"[*] ëª¨ë¸ ë¡œë”© ì‹œì‘: {args.ckpt_dir}")
    cleanup()

    # Wan ëª¨ë¸ ì„¤ì • ë¡œë“œ
    cfg = WAN_CONFIGS[args.task]
    
    # 2. ëª¨ë¸ ë¡œë”© (í•µì‹¬ ìµœì í™” ë¶€ë¶„)
    # low_cpu_mem_usage=True: ê°€ì¤‘ì¹˜ë¥¼ RAMì— í•œêº¼ë²ˆì— ì˜¬ë¦¬ì§€ ì•Šê³  ìˆœì°¨ì ìœ¼ë¡œ ë¡œë“œ
    # device_map="auto": VRAMê³¼ RAM ì‚¬ì´ì˜ ë°°ì¹˜ë¥¼ ìë™ ìµœì í™”
    try:
        model = WanModel.from_pretrained(
            args.ckpt_dir,
            low_cpu_mem_usage=True,
            device_map="auto", 
            torch_dtype=torch.bfloat16, # T5ì™€ DiT ëª¨ë‘ BF16ìœ¼ë¡œ ê³ ì •í•˜ì—¬ RAM ì ˆì•½
            variant="fp16" # ë˜ëŠ” "nf4" ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
        )
    except Exception as e:
        print(f"[!] ë¡œë”© ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        print("[*] device_mapì„ 'balanced'ë¡œ ë³€ê²½í•˜ì—¬ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
        model = WanModel.from_pretrained(
            args.ckpt_dir,
            low_cpu_mem_usage=True,
            device_map="balanced",
            torch_dtype=torch.bfloat16
        )

    # 3. T5 ì¸ì½”ë” ê°•ì œ CPU ì˜¤í”„ë¡œë”© (3090 VRAM ë¶€ì¡± ë°©ì§€)
    if args.t5_cpu and hasattr(model, 'text_encoder'):
        print("[*] T5 ì¸ì½”ë”ë¥¼ CPUë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        model.text_encoder.to("cpu")
        cleanup()

    # 4. ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    img = Image.open(args.image).convert("RGB")
    width, height = map(int, args.size.split("*"))
    
    print(f"[*] ìƒì„± ì‹œì‘: {args.prompt} ({args.size}, {args.frame_num} frames)")

    # 5. ë¹„ë””ì˜¤ ìƒì„± ì‹¤í–‰
    with torch.no_grad():
        video = model.generate(
            input_image=img,
            prompt=args.prompt,
            size=(width, height),
            frame_num=args.frame_num,
            shift=5.0, # ëª¨ë¸ íŠ¹ì„±ì— ë”°ë¥¸ ì„¤ì •ê°’
            sampling_steps=40
        )

    # 6. ê²°ê³¼ ì €ì¥
    output_path = "output_video.mp4"
    cache_video(video, output_path, fps=16)
    print(f"[*] ìƒì„± ì™„ë£Œ: {output_path}")

if __name__ == "__main__":
    # bitsandbytes í™˜ê²½ ë³€ìˆ˜ ê°•ì œ ì ìš©
    os.environ["BNB_CUDA_VERSION"] = "124"
    generate()

```

---

### ğŸš€ ì‹¤í–‰ ì‹œ ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì‚¬í•­

**1. ì‹¤í–‰ ëª…ë ¹ì–´ (í„°ë¯¸ë„)**
ë°˜ë“œì‹œ `--t5_cpu True` ì˜µì…˜ì„ ëª…ì‹œí•˜ì—¬ ì‹¤í–‰í•¨.

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” ìƒíƒœì—ì„œ ì‹¤í–‰
export BNB_CUDA_VERSION=124
python generate_prequant.py \
    --ckpt_dir ./models/lingbot-nf4 \
    --image examples/00/image.jpg \
    --prompt "A professional drone shot of a futuristic city, high fidelity" \
    --t5_cpu True \
    --size 480*832 \
    --frame_num 81

```

**2. ì½”ë“œ ë‚´ í•µì‹¬ ë³€ê²½ì  ì„¤ëª…**

* **`low_cpu_mem_usage=True`**: ì´ ì˜µì…˜ì´ ì—†ìœ¼ë©´ `torch`ëŠ” íŒŒì¼ í¬ê¸°ì˜ 2~3ë°°ì— ë‹¬í•˜ëŠ” RAMì„ ì ìœ í•¨. 64GB RAMì´ ê½‰ ì°¨ì„œ ì£½ëŠ” í˜„ìƒì„ ë§‰ì•„ì£¼ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ì½”ë“œì„.
* **`device_map="auto"`**: ëª¨ë¸ì˜ ë ˆì´ì–´ë“¤ì„ 3090 VRAM(24GB)ì— ìš°ì„ ì ìœ¼ë¡œ ì±„ìš°ê³ , ë„˜ì¹˜ëŠ” ë¶€ë¶„(T5 ë“±)ì„ ìë™ìœ¼ë¡œ ì‹œìŠ¤í…œ RAMìœ¼ë¡œ ë³´ëƒ„.
* **`torch_dtype=torch.bfloat16`**: ëª¨ë¸ì„ 32ë¹„íŠ¸(Full Precision)ë¡œ ì½ì§€ ì•Šë„ë¡ ê°•ì œí•¨. 32ë¹„íŠ¸ë¡œ ì½í ê²½ìš° 64GB RAMìœ¼ë¡œë„ ë¶€ì¡±í•  ìˆ˜ ìˆìŒ.

### âš ï¸ ë§Œì•½ ì—¬ì „íˆ ì£½ëŠ”ë‹¤ë©´?

1. **ìŠ¤ì™‘(Swap) í™•ì¸:** ì•„ê¹Œ ì„¤ì •í•œ 32GB ìŠ¤ì™‘ì´ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸í•¨ (`free -h`).
2. **í•´ìƒë„ í•˜í–¥:** `--size 240*432`ë¡œ ë‚®ì¶”ì–´ ëª¨ë¸ ë¡œë”© ì´í›„ì˜ ìƒì„± ë‹¨ê³„ì—ì„œ ì£½ëŠ”ì§€ í™•ì¸í•¨.
3. **T5 íŒŒì¼ ë¶„ë¦¬:** ë§Œì•½ ë¡œë”© ìì²´ê°€ ì•ˆ ëœë‹¤ë©´, ëª¨ë¸ í´ë”ì—ì„œ T5 ê´€ë ¨ íŒŒì¼ì„ ì ì‹œ ë¹¼ë‘ê³  DiT ëª¨ë¸ë§Œ ë¡œë“œë˜ëŠ”ì§€ í™•ì¸í•˜ì—¬ ë²”ì¸ì„ ì°¾ì•„ì•¼ í•¨.

**ì´ ì½”ë“œë¥¼ ì ìš©í•œ ë’¤ í„°ë¯¸ë„ ë¡œê·¸ì— "Loading checkpoint shards"ë¼ëŠ” ë©”ì‹œì§€ê°€ ë‚˜ì˜¤ë©´ì„œ ì§„í–‰ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì‹œê¸° ë°”ëŒ.** ì´ë²ˆì—ëŠ” 64GB RAM ì•ˆì—ì„œ ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ë  ê²ƒì„. ì¶”ê°€ ì—ëŸ¬ ë°œìƒ ì‹œ ë¡œê·¸ë¥¼ ë³µì‚¬í•´ ì£¼ì‹œë©´ ì¦‰ì‹œ ëŒ€ì‘í•´ ë“œë¦¼.

Would you like me to explain how to further optimize the T5 encoder if it still takes too much RAM?
