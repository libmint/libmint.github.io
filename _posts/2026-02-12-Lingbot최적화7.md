오케이. “서서히” 말고 **한 방에(=64GB에서 확실히 통과 확률 올리는 구조)**로 가려면, 제일 강한 카드는 이겁니다:

✅ **T5 인코딩을 ‘별도 프로세스’에서 수행 → 결과만 파일로 넘기고 프로세스 종료(메모리 통째로 반환)**
리눅스는 같은 프로세스에서 `del/gc` 해도 RSS가 잘 안 내려가는 경우가 많아요. **프로세스를 끝내는 게 가장 확실**합니다.

아래는 **복붙해서 바로 쓰는 최종 2파일**입니다.

* `load_prequant.py` : (기존대로) **safetensors 스트리밍 + 가능하면 init_empty/to_empty**
* `generate_prequant_64gb_oneshot.py` : **T5를 별도 프로세스로 돌려서 RAM “한 방에” 절약**

---

## 0) 실행 전 export (복붙)

```bash
export MALLOC_ARENA_MAX=2
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
```

---

## 1) `load_prequant.py` (전체 복붙)

> 이전에 드린 것과 동일한 “가장 안전한” 버전입니다.

```python
#!/usr/bin/env python3
import json
import os
import sys
import gc
from pathlib import Path
from typing import Dict, Any, Tuple

import torch
import torch.nn as nn
import bitsandbytes as bnb
from bitsandbytes.functional import QuantState
from safetensors import safe_open

try:
    from accelerate import init_empty_weights  # type: ignore
    _HAS_ACCELERATE = True
except Exception:
    _HAS_ACCELERATE = False

sys.path.insert(0, str(Path(__file__).parent))
from wan.modules.model import WanModel


def _get_compute_dtype(config: Dict[str, Any]) -> torch.dtype:
    cd = str(config.get("compute_dtype", "bfloat16")).lower()
    if cd in ("float16", "fp16"):
        return torch.float16
    return torch.bfloat16


def build_model_from_config(config: Dict[str, Any]) -> WanModel:
    return WanModel(
        model_type=config.get("model_type", "i2v"),
        patch_size=tuple(config.get("patch_size", (1, 2, 2))),
        text_len=config.get("text_len", 512),
        in_dim=config.get("in_dim", 16),
        dim=config.get("dim", 2048),
        ffn_dim=config.get("ffn_dim", 8192),
        freq_dim=config.get("freq_dim", 256),
        text_dim=config.get("text_dim", 4096),
        out_dim=config.get("out_dim", 16),
        num_heads=config.get("num_heads", 16),
        num_layers=config.get("num_layers", 32),
        window_size=tuple(config.get("window_size", (-1, -1))),
        qk_norm=config.get("qk_norm", True),
        cross_attn_norm=config.get("cross_attn_norm", True),
        eps=config.get("eps", 1e-6),
    )


def replace_linears_with_bnb_nf4(
    model: nn.Module,
    compute_dtype: torch.dtype = torch.bfloat16,
    compress_statistics: bool = True,
    quant_type: str = "nf4",
) -> Tuple[int, Dict[str, Tuple[int, int]]]:
    replaced = 0
    layer_shapes: Dict[str, Tuple[int, int]] = {}
    linear_layers = [(n, m) for n, m in model.named_modules() if isinstance(m, nn.Linear)]

    for name, module in linear_layers:
        parent_name = ".".join(name.split(".")[:-1])
        child_name = name.split(".")[-1]
        parent = model.get_submodule(parent_name) if parent_name else model

        layer_shapes[name] = (module.in_features, module.out_features)

        nf4_linear = bnb.nn.Linear4bit(
            module.in_features,
            module.out_features,
            bias=module.bias is not None,
            compute_dtype=compute_dtype,
            compress_statistics=compress_statistics,
            quant_type=quant_type,
        )
        setattr(parent, child_name, nf4_linear)
        replaced += 1

    return replaced, layer_shapes


def reconstruct_params4bit_from_components(comps: Dict[str, torch.Tensor], device: str) -> bnb.nn.Params4bit:
    qs_dict = {
        "absmax": comps["absmax"],
        "quant_map": comps["quant_map"],
        "quant_state.bitsandbytes__nf4": comps["quant_state_data"],
    }
    if "nested_absmax" in comps:
        qs_dict["nested_absmax"] = comps["nested_absmax"]
        qs_dict["nested_quant_map"] = comps["nested_quant_map"]

    quant_state = QuantState.from_dict(qs_dict, device=torch.device(device))
    return bnb.nn.Params4bit(
        data=comps["weight"],
        requires_grad=False,
        quant_state=quant_state,
        bnb_quantized=True,
    )


def _stream_load_safetensors(
    model: nn.Module,
    weights_path: str,
    layer_shapes: Dict[str, Tuple[int, int]],
    device: str,
) -> nn.Module:
    st_device = device if device.startswith("cuda") else "cpu"

    param_map = dict(model.named_parameters())
    buffer_map = dict(model.named_buffers())
    linear4bit_names = {n for n, m in model.named_modules() if isinstance(m, bnb.nn.Linear4bit)}

    def is_quant_component(base: str, k: str) -> bool:
        return k in {
            f"{base}.weight",
            f"{base}.weight.absmax",
            f"{base}.weight.quant_map",
            f"{base}.weight.nested_absmax",
            f"{base}.weight.nested_quant_map",
            f"{base}.weight.quant_state.bitsandbytes__nf4",
        }

    loaded = 0
    with safe_open(weights_path, framework="pt", device=st_device) as f:
        keys = set(f.keys())

        # 1) quantized Linear4bit layers
        for name, module in model.named_modules():
            if not isinstance(module, bnb.nn.Linear4bit):
                continue
            if name not in layer_shapes:
                continue

            k_w = f"{name}.weight"
            if k_w not in keys:
                continue

            k_abs = f"{name}.weight.absmax"
            k_qmap = f"{name}.weight.quant_map"
            k_qstate = f"{name}.weight.quant_state.bitsandbytes__nf4"
            missing = [k for k in (k_abs, k_qmap, k_qstate) if k not in keys]
            if missing:
                raise RuntimeError(f"Missing quant components for {name}: {missing}")

            comps = {
                "weight": f.get_tensor(k_w),
                "absmax": f.get_tensor(k_abs),
                "quant_map": f.get_tensor(k_qmap),
                "quant_state_data": f.get_tensor(k_qstate),
            }

            k_nabs = f"{name}.weight.nested_absmax"
            k_nq = f"{name}.weight.nested_quant_map"
            if k_nabs in keys and k_nq in keys:
                comps["nested_absmax"] = f.get_tensor(k_nabs)
                comps["nested_quant_map"] = f.get_tensor(k_nq)

            module.weight = reconstruct_params4bit_from_components(comps, device=device)
            loaded += 1

            k_b = f"{name}.bias"
            if k_b in keys and module.bias is not None:
                module.bias.data.copy_(f.get_tensor(k_b))

            del comps
            if loaded % 64 == 0:
                gc.collect()

        print(f"Loaded {loaded} quantized Linear4bit layers (streaming, st_device={st_device})")

        # 2) remaining (non-quant) params/buffers
        for k in keys:
            skip = False
            for ln in linear4bit_names:
                if k.startswith(ln + ".") and (is_quant_component(ln, k) or k == f"{ln}.bias"):
                    skip = True
                    break
            if skip:
                continue

            t = f.get_tensor(k)

            if k in param_map:
                dst = param_map[k]
                if dst.device.type == "meta":
                    raise RuntimeError(f"Param {k} is meta (to_empty/init_empty_weights not applied correctly).")
                if dst.shape != t.shape:
                    raise RuntimeError(f"Shape mismatch for {k}: {tuple(dst.shape)} vs {tuple(t.shape)}")
                dst.data.copy_(t if t.device == dst.device else t.to(dst.device))
            elif k in buffer_map:
                dst = buffer_map[k]
                if dst.device.type == "meta":
                    raise RuntimeError(f"Buffer {k} is meta (to_empty/init_empty_weights not applied correctly).")
                if dst.shape != t.shape:
                    raise RuntimeError(f"Shape mismatch for buffer {k}: {tuple(dst.shape)} vs {tuple(t.shape)}")
                dst.data.copy_(t if t.device == dst.device else t.to(dst.device))

            del t

    gc.collect()
    return model


def load_quantized_model(model_dir: str, device: str = "cpu") -> WanModel:
    model_dir = str(model_dir)
    cfg_path = os.path.join(model_dir, "config.json")
    if not os.path.exists(cfg_path):
        raise FileNotFoundError(f"Missing config.json: {cfg_path}")

    with open(cfg_path, "r", encoding="utf-8") as f:
        config = json.load(f)

    compute_dtype = _get_compute_dtype(config)

    if _HAS_ACCELERATE:
        with init_empty_weights():
            model = build_model_from_config(config)
    else:
        model = build_model_from_config(config)

    model.eval()

    replaced, layer_shapes = replace_linears_with_bnb_nf4(
        model,
        compute_dtype=compute_dtype,
        compress_statistics=bool(config.get("double_quant", True)),
        quant_type=str(config.get("quant_type", "nf4")),
    )
    print(f"Replaced {replaced} Linear layers with Linear4bit shells")

    if hasattr(model, "to_empty"):
        model = model.to_empty(device=device)  # type: ignore
    else:
        model = model.to(device)

    weights_path = None
    for cand in ("model.safetensors", "model.pt", "pytorch_model.bin"):
        p = os.path.join(model_dir, cand)
        if os.path.exists(p):
            weights_path = p
            break
    if weights_path is None:
        raise FileNotFoundError(f"No weights found in {model_dir} (expected model.safetensors)")

    if weights_path.endswith(".safetensors"):
        model = _stream_load_safetensors(model, weights_path, layer_shapes, device=device)
    else:
        sd = torch.load(weights_path, map_location="cpu", weights_only=False)
        model.load_state_dict(sd, strict=False)
        del sd

    gc.collect()
    return model
```

---

## 2) `generate_prequant_64gb_oneshot.py` (전체 복붙)

> **핵심**: T5는 `multiprocessing`으로 **자식 프로세스에서만 로드/인코딩**하고 종료합니다.
> 메인 프로세스는 T5를 아예 들고 있지 않아서 **RSS가 확 내려갑니다.**

```python
#!/usr/bin/env python3
import argparse
import gc
import logging
import os
import random
import sys
import tempfile
from pathlib import Path
import multiprocessing as mp

import numpy as np
import torch
import torchvision.transforms.functional as TF
from PIL import Image
from tqdm import tqdm

sys.path.insert(0, str(Path(__file__).parent))

from einops import rearrange
from load_prequant import load_quantized_model
from wan.configs.wan_i2v_A14B import i2v_A14B as cfg
from wan.modules.t5 import T5EncoderModel
from wan.modules.vae2_1 import Wan2_1_VAE
from wan.utils.cam_utils import (
    compute_relative_poses,
    get_Ks_transformed,
    get_plucker_embeddings,
    interpolate_camera_poses,
)
from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

torch.backends.cuda.matmul.allow_tf32 = True
try:
    torch.backends.cuda.enable_flash_sdp(True)
    torch.backends.cuda.enable_mem_efficient_sdp(True)
    torch.backends.cuda.enable_math_sdp(False)
except Exception:
    pass


def _t5_encode_worker(payload, out_path: str):
    """
    Runs in a separate process.
    Loads T5, encodes prompts, saves CPU tensors to out_path, then exits.
    """
    ckpt_dir = payload["ckpt_dir"]
    prompt = payload["prompt"]
    neg = payload["neg"]
    text_len = payload["text_len"]
    t5_dtype = payload["t5_dtype"]
    t5_ckpt = payload["t5_checkpoint"]
    t5_tokenizer = payload["t5_tokenizer"]

    local_tokenizer = os.path.join(ckpt_dir, "tokenizer")
    tokenizer_path = local_tokenizer if os.path.isdir(local_tokenizer) else t5_tokenizer

    text_encoder = T5EncoderModel(
        text_len=text_len,
        dtype=t5_dtype,
        device=torch.device("cpu"),
        checkpoint_path=os.path.join(ckpt_dir, t5_ckpt),
        tokenizer_path=tokenizer_path,
        shard_fn=None,
    )

    # encode on CPU
    ctx = text_encoder([prompt], torch.device("cpu"))
    ctx0 = text_encoder([neg], torch.device("cpu"))

    # save as CPU tensors (float16/bf16 유지)
    torch.save(
        {"context": ctx, "context_null": ctx0},
        out_path,
        _use_new_zipfile_serialization=False,
    )


class WanI2V_PreQuantOneShot64GB:
    def __init__(self, checkpoint_dir: str, device_id: int = 0):
        self.device = torch.device(f"cuda:{device_id}")

        self.num_train_timesteps = cfg.num_train_timesteps
        self.boundary = cfg.boundary
        self.param_dtype = cfg.param_dtype
        self.vae_stride = cfg.vae_stride
        self.patch_size = cfg.patch_size
        self.sample_neg_prompt = cfg.sample_neg_prompt

        logger.info("Loading VAE...")
        self.vae = Wan2_1_VAE(
            vae_pth=os.path.join(checkpoint_dir, cfg.vae_checkpoint),
            device=self.device,
        )

        self.low_noise_dir = os.path.join(checkpoint_dir, cfg.low_noise_checkpoint + "_bnb_nf4")
        self.high_noise_dir = os.path.join(checkpoint_dir, cfg.high_noise_checkpoint + "_bnb_nf4")
        for d in (self.low_noise_dir, self.high_noise_dir):
            if not os.path.isdir(d):
                raise FileNotFoundError(f"Missing prequant model dir: {d}")

        self._loaded_name = None
        self._model = None

    def _unload_model(self):
        if self._model is not None:
            try:
                self._model.to("cpu")
            except Exception:
                pass
            del self._model
            self._model = None
        self._loaded_name = None
        gc.collect()
        torch.cuda.empty_cache()

    def _load_model(self, which: str):
        assert which in ("low", "high")
        if self._loaded_name == which and self._model is not None:
            return self._model

        self._unload_model()
        model_dir = self.low_noise_dir if which == "low" else self.high_noise_dir
        logger.info(f"Loading {which} diffusion model to GPU: {model_dir}")

        # IMPORTANT: load directly to cuda
        self._model = load_quantized_model(model_dir, device=str(self.device))
        self._model.eval()

        self._loaded_name = which
        gc.collect()
        torch.cuda.empty_cache()
        return self._model

    def _prepare_model_for_timestep(self, t, boundary):
        return self._load_model("high" if t.item() >= boundary else "low")

    def _encode_text_in_subprocess(self, ckpt_dir: str, prompt: str, neg: str):
        """
        One-shot memory killer:
        T5 runs in a child process; parent only loads the tiny output tensors.
        """
        payload = {
            "ckpt_dir": ckpt_dir,
            "prompt": prompt,
            "neg": neg,
            "text_len": cfg.text_len,
            "t5_dtype": cfg.t5_dtype,
            "t5_checkpoint": cfg.t5_checkpoint,
            "t5_tokenizer": cfg.t5_tokenizer,
        }

        with tempfile.TemporaryDirectory() as td:
            out_path = os.path.join(td, "t5_ctx.pt")
            p = mp.Process(target=_t5_encode_worker, args=(payload, out_path), daemon=True)
            p.start()
            p.join()

            if p.exitcode != 0:
                raise RuntimeError(f"T5 subprocess failed with exitcode={p.exitcode}")

            pack = torch.load(out_path, map_location="cpu")
            ctx = pack["context"]
            ctx0 = pack["context_null"]

        # move to GPU in parent (small tensors compared to T5 weights)
        ctx = [t.to(self.device) for t in ctx]
        ctx0 = [t.to(self.device) for t in ctx0]
        return ctx, ctx0

    def generate(
        self,
        ckpt_dir: str,
        input_prompt: str,
        img: Image.Image,
        action_path: str = None,
        max_area: int = 480 * 832,
        frame_num: int = 81,
        shift: float = 5.0,
        sampling_steps: int = 40,
        guide_scale: float = 5.0,
        n_prompt: str = "",
        seed: int = -1,
    ):
        if n_prompt == "":
            n_prompt = self.sample_neg_prompt

        # ✅ T5 in separate process (big RAM saver)
        context, context_null = self._encode_text_in_subprocess(ckpt_dir, input_prompt, n_prompt)
        gc.collect()

        if action_path is not None:
            c2ws = np.load(os.path.join(action_path, "poses.npy"))
            len_c2ws = ((len(c2ws) - 1) // 4) * 4 + 1
            frame_num = min(frame_num, len_c2ws)
            c2ws = c2ws[:frame_num]

        img_tensor = TF.to_tensor(img).sub_(0.5).div_(0.5).to(self.device)

        F = frame_num
        h0, w0 = img_tensor.shape[1:]
        aspect_ratio = h0 / w0

        lat_h = round(np.sqrt(max_area * aspect_ratio) // self.vae_stride[1] // self.patch_size[1] * self.patch_size[1])
        lat_w = round(np.sqrt(max_area / aspect_ratio) // self.vae_stride[2] // self.patch_size[2] * self.patch_size[2])

        h = lat_h * self.vae_stride[1]
        w = lat_w * self.vae_stride[2]
        lat_f = (F - 1) // self.vae_stride[0] + 1
        max_seq_len = lat_f * lat_h * lat_w // (self.patch_size[1] * self.patch_size[2])

        seed = seed if seed >= 0 else random.randint(0, sys.maxsize)
        seed_g = torch.Generator(device=self.device).manual_seed(seed)

        noise = torch.randn(
            16,
            (F - 1) // self.vae_stride[0] + 1,
            lat_h,
            lat_w,
            dtype=self.param_dtype,
            generator=seed_g,
            device=self.device,
        )

        msk = torch.ones(1, F, lat_h, lat_w, device=self.device)
        msk[:, 1:] = 0
        msk = torch.concat([torch.repeat_interleave(msk[:, 0:1], repeats=4, dim=1), msk[:, 1:]], dim=1)
        msk = msk.view(1, msk.shape[1] // 4, 4, lat_h, lat_w).transpose(1, 2)[0]

        dit_cond_dict = None
        if action_path is not None:
            Ks = torch.from_numpy(np.load(os.path.join(action_path, "intrinsics.npy"))).float()
            Ks = get_Ks_transformed(Ks, 480, 832, h, w, h, w)[0].to(self.device)

            c2ws_infer = interpolate_camera_poses(
                src_indices=np.linspace(0, len(c2ws) - 1, len(c2ws)),
                src_rot_mat=c2ws[:, :3, :3],
                src_trans_vec=c2ws[:, :3, 3],
                tgt_indices=np.linspace(0, len(c2ws) - 1, int((len(c2ws) - 1) // 4) + 1),
            )
            c2ws_infer = compute_relative_poses(c2ws_infer, framewise=True).to(self.device)

            c2ws_plucker_emb = get_plucker_embeddings(c2ws_infer, Ks.repeat(len(c2ws_infer), 1), h, w)
            c2ws_plucker_emb = rearrange(
                c2ws_plucker_emb,
                "f (h c1) (w c2) c -> (f h w) (c c1 c2)",
                c1=int(h // lat_h),
                c2=int(w // lat_w),
            )
            c2ws_plucker_emb = c2ws_plucker_emb[None, ...]
            c2ws_plucker_emb = rearrange(
                c2ws_plucker_emb,
                "b (f h w) c -> b c f h w",
                f=lat_f,
                h=lat_h,
                w=lat_w,
            ).to(self.param_dtype)
            dit_cond_dict = {"c2ws_plucker_emb": c2ws_plucker_emb.chunk(1, dim=0)}

        img_resized = torch.nn.functional.interpolate(img_tensor[None], size=(h, w), mode="bicubic").transpose(0, 1)
        img_resized = img_resized.to(dtype=self.param_dtype)
        zeros = torch.zeros(3, F - 1, h, w, device=self.device, dtype=self.param_dtype)

        y = self.vae.encode([torch.concat([img_resized, zeros], dim=1)])[0].to(dtype=self.param_dtype)
        y = torch.concat([msk, y])

        with torch.amp.autocast("cuda", dtype=self.param_dtype), torch.inference_mode():
            boundary = self.boundary * self.num_train_timesteps
            scheduler = FlowUniPCMultistepScheduler(
                num_train_timesteps=self.num_train_timesteps,
                shift=1,
                use_dynamic_shifting=False,
            )
            scheduler.set_timesteps(sampling_steps, device=self.device, shift=shift)
            timesteps = scheduler.timesteps

            latent = noise
            arg_c = {"context": [context[0]], "seq_len": max_seq_len, "y": [y], "dit_cond_dict": dit_cond_dict}
            arg_null = {"context": context_null, "seq_len": max_seq_len, "y": [y], "dit_cond_dict": dit_cond_dict}

            for t in tqdm(timesteps, desc="Sampling"):
                model = self._prepare_model_for_timestep(t, boundary)

                timestep = t[None].to(self.device)
                latent_in = [latent]

                uncond = model(latent_in, t=timestep, **arg_null)[0]
                cond = model(latent_in, t=timestep, **arg_c)[0]

                # in-place CFG (VRAM peak 감소)
                cond.sub_(uncond)
                cond.mul_(guide_scale)
                cond.add_(uncond)
                del uncond

                latent = scheduler.step(
                    cond.unsqueeze(0),
                    t,
                    latent.unsqueeze(0),
                    return_dict=False,
                    generator=seed_g,
                )[0].squeeze(0)
                del cond

            self._unload_model()
            videos = self.vae.decode([latent])

        del noise, latent, zeros, img_resized, y, context, context_null
        gc.collect()
        torch.cuda.synchronize()
        return videos[0]


def save_video(frames: torch.Tensor, output_path: str, fps: int = 16):
    import imageio
    frames = ((frames + 1) / 2 * 255).clamp(0, 255).byte()
    frames = frames.permute(1, 2, 3, 0).cpu().numpy()
    imageio.mimwrite(output_path, frames, fps=fps, codec="libx264")
    logger.info(f"Saved video to {output_path}")


def main():
    mp.set_start_method("spawn", force=True)

    p = argparse.ArgumentParser()
    script_dir = str(Path(__file__).parent)

    p.add_argument("--ckpt_dir", type=str, default=script_dir)
    p.add_argument("--image", type=str, required=True)
    p.add_argument("--prompt", type=str, required=True)
    p.add_argument("--action_path", type=str, default=None)
    p.add_argument("--size", type=str, default="480*832")
    p.add_argument("--frame_num", type=int, default=81)
    p.add_argument("--sampling_steps", type=int, default=40)
    p.add_argument("--guide_scale", type=float, default=5.0)
    p.add_argument("--seed", type=int, default=-1)
    p.add_argument("--output", type=str, default="out.mp4")

    args = p.parse_args()
    h, w = map(int, args.size.split("*"))
    max_area = h * w

    img = Image.open(args.image).convert("RGB")
    pipe = WanI2V_PreQuantOneShot64GB(checkpoint_dir=args.ckpt_dir)

    vid = pipe.generate(
        ckpt_dir=args.ckpt_dir,
        input_prompt=args.prompt,
        img=img,
        action_path=args.action_path,
        max_area=max_area,
        frame_num=args.frame_num,
        sampling_steps=args.sampling_steps,
        guide_scale=args.guide_scale,
        seed=args.seed,
    )
    save_video(vid, args.output)


if __name__ == "__main__":
    main()
```

---

## 3) 실행 (복붙)

```bash
python generate_prequant_64gb_oneshot.py \
  --ckpt_dir . \
  --image examples/00/image.jpg \
  --prompt "A cinematic video of the scene" \
  --frame_num 81 \
  --sampling_steps 40 \
  --size 480*832 \
  --output out.mp4
```

---

### 이게 “한방”인 이유

* T5(텍스트 인코더)가 RAM을 크게 잡는 구간이 있는데, 같은 프로세스에서 지워도 RSS가 안 내려가는 경우가 많습니다.
* **별도 프로세스로 돌리고 종료**하면, 커널이 메모리를 **통째로 반환**합니다. (가장 확실)

실행해보고 또 죽으면, 그 즉시 `dmesg -T | tail -n 40`에서 `anon-rss`만 확인해서 “다음 한 방”(예: **VAE encode도 별도 프로세스**, 혹은 **diffusion 모델 로딩을 완전 분리**)까지 바로 밀어붙여드릴게요.
