NEULAB의 **UIX-Qwen2** 모델을 사용하기 위한 단계별 가이드입니다. 초보자를 위해 설치부터 실행까지 상세히 설명드리겠습니다.

---

### **1. 필수 패키지 설치**
```bash
# 파이썬 3.10 이상 권장
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # CUDA 11.8 기준
pip install transformers accelerate pillow
```

---

### **2. 기본 사용 코드**
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image

# 모델 및 토크나이저 로드
model = AutoModelForCausalLM.from_pretrained(
    "neulab/UIX-Qwen2",
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained("neulab/UIX-Qwen2")

# 이미지 준비
image = Image.open("이미지_경로.jpg").convert("RGB")

# 프롬프트 생성
prompt = "이 웹페이지에서 주요 버튼은 어디에 있나요?"
inputs = tokenizer(prompt, images=image, return_tensors="pt").to(model.device)

# 추론 실행
outputs = model.generate(**inputs, max_new_tokens=200)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---

### **3. 고급 기능 활용**
**다중 이미지 처리**:
```python
images = [Image.open(f"이미지_{i}.jpg") for i in range(3)]
inputs = tokenizer(
    "이 이미지들에서 공통된 디자인 요소를 찾아주세요",
    images=images,
    return_tensors="pt",
    padding=True
).to(model.device)
```

**비디오 분석** (베타 기능):
```python
from decord import VideoReader

vr = VideoReader("비디오.mp4")
frames = [Image.fromarray(vr[i].asnumpy()) for i in range(0, len(vr), 30)]  # 30프레임 간격 샘플링

inputs = tokenizer(
    "이 영상에서 주된 행동은 무엇인가요?",
    images=frames,
    return_tensors="pt",
    padding=True
).to(model.device)
```

---

### **4. 주의 사항**
1. **하드웨어 요구사항**:
   - 7B 모델: 최소 16GB VRAM (24GB 권장)
   - CPU 모드: `device_map="cpu"` 설정 가능(속도 저하)

2. **이미지 규격**:
   ```python
   # 권장 해상도: 672x672 픽셀
   image = image.resize((672, 672))
   ```

3. **에러 대처**:
   - `CUDA out of memory`: `max_new_tokens` 값 감소 시도
   - `TrustRemoteCode` 경고: 공식 리포지토리 클론 후 로컬에서 실행

---

### **5. 배포 옵션**
**Gradio 웹 인터페이스**:
```python
import gradio as gr

def analyze(image, question):
    inputs = tokenizer(question, images=image, return_tensors="pt")
    outputs = model.generate(**inputs)
    return tokenizer.decode(outputs[0])

gr.Interface(
    fn=analyze,
    inputs=[gr.Image(type="pil"), gr.Textbox()],
    outputs="text"
).launch()
```

**배치 처리**:
```python
from datasets import load_dataset

dataset = load_dataset("your_dataset")
results = []
for item in dataset:
    inputs = tokenizer(item["text"], images=item["image"], return_tensors="pt", padding=True)
    outputs = model.generate(**inputs)
    results.append(tokenizer.decode(outputs[0]))
```

---
