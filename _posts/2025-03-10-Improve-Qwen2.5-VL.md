ì•„ë˜ëŠ” ì›ë³¸ ì½”ë“œì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ìœ ì§€í•˜ë©´ì„œ, ëª¨ë¸ë§Œ **Qwen2.5-VL**ë¡œ ë³€ê²½í•˜ì—¬ ìˆ˜ì •í•œ ì „ì²´ ì½”ë“œì…ë‹ˆë‹¤.

### âœ… ìµœì¢… ìˆ˜ì •ëœ ì½”ë“œ (Qwen2.5-VL ëª¨ë¸ ì‚¬ìš©)

```python
# -*- coding: utf-8 -*-

import os
import re
import torch
import pickle
import argparse
from fastapi import FastAPI
from pydantic import BaseModel
import base64
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import uvicorn
from fastapi.middleware.cors import CORSMiddleware

parser = argparse.ArgumentParser()
parser.add_argument("--device", type=str, default="cuda:0")
parser.add_argument("--model_name_or_path", type=str, default="Qwen/Qwen2.5-VL-7B-Instruct")
parser.add_argument("--port", type=int, default=8080)
args = parser.parse_args()

print("Loaded model:", args.model_name_or_path)

# Processor ë¡œë”© (Qwen2.5-VL)
processor = AutoProcessor.from_pretrained(
    args.model_name_or_path,
    do_image_splitting=False,
    min_pixels=256*28*28,
    max_pixels=1280*28*28
)

# Data Collator ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼í•œ ê¸°ëŠ¥)
class MyDataCollator:
    def __init__(self, processor):
        self.processor = processor
        self.image_token_id = processor.tokenizer.convert_tokens_to_ids("")

    def __call__(self, example_list):
        texts = []
        images = []
        for example in example_list:
            image_list = example["images"]

            messages = []
            conversations = example["conversations"]
            for conv in conversations:
                item = {}
                item["role"] = conv["role"]
                raw_content = conv["content"]
                raw_content_split = re.split(r'()', raw_content)
                content_list = [
                    {"type": "image"} if seg == "" else {"type": "text", "text": seg}
                    for seg in raw_content_split if seg.strip()
                ]
                item = {"role": conv["role"], "content": content_list}
                messages.append(item)

            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            texts.append(text.strip())
            images.append(image_list)

        batch = processor(text=texts, images=images, return_tensors="pt", padding=True)
        return batch

# ëª¨ë¸ ë¡œë”© (Qwen2.5-VL)
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    args.model_name_or_path,
    torch_dtype=torch.float16,
).to(args.device)

data_collator = MyDataCollator(processor)

class InputData(BaseModel):
    id: str
    conversations: list
    images: str  # base64ë¡œ ì¸ì½”ë”©ëœ pickle.dumps(image_list)ì˜ ê²°ê³¼ë¬¼

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/predict")
def predict(example: InputData):
    example_dict = example.dict()

    # ì´ë¯¸ì§€ ë””ì½”ë”© ë° PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    image_list_bin = base64.b64decode(example_dict["images"])
    image_list_pil = pickle.loads(image_list_bin)

    converted_example = {
        "id": example_dict["id"],
        "images": image_list_pil,
        "conversations": example_dict["conversations"]
    }

    batch = data_collator([converted_example])
    batch = {k: v.to(args.device) for k, v in batch.items()}

    with torch.no_grad():
        generated_ids = model.generate(
            **batch,
            max_new_tokens=256,
            min_new_tokens=3,
            eos_token_id=processor.tokenizer.eos_token_id,
            do_sample=True,
            temperature=1.2,
        )

    generated_text = processor.batch_decode(
        generated_ids[:, batch["input_ids"].size(1):],
        skip_special_tokens=True
    )[0]

    input_token_count = batch["input_ids"].size(1)
    output_token_count = generated_ids.size(1) - input_token_count

    return {
        "text": generated_text,
        "prompt_tokens": input_token_count,
        "completion_tokens": output_token_count
    }

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/predict")
def predict_endpoint(example: InputData):
    return predict(example)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=args.port)
```

---

### ğŸ“Œ ì£¼ìš” ìˆ˜ì • ì‚¬í•­ ìš”ì•½:

- **ëª¨ë¸ ë³€ê²½:** ê¸°ì¡´ì˜ `Idefics2ForConditionalGeneration`ì„ `Qwen2_5_VLForConditionalGeneration`ìœ¼ë¡œ êµì²´í–ˆìŠµë‹ˆë‹¤.
- **Processor ë³€ê²½**: `AutoProcessor`ë¥¼ Qwen2.5-VL ëª¨ë¸ì— ë§ê²Œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.
- **í† í° ì²˜ë¦¬ ë°©ì‹ ìœ ì§€**: ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ `` í† í°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- **ì…ë ¥ ë°ì´í„° í˜•ì‹ ìœ ì§€**: ê¸°ì¡´ì˜ Base64 ì¸ì½”ë”©ëœ pickle ì´ë¯¸ì§€ ì…ë ¥ ë°©ì‹ì„ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.
- **FastAPI ì¸í„°í˜ì´ìŠ¤ ìœ ì§€**: ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ ì™„ì „íˆ ë™ì¼í•˜ê²Œ ìœ ì§€í–ˆìŠµë‹ˆë‹¤.

---

### ğŸš© í…ŒìŠ¤íŠ¸ ë°©ë²• ì˜ˆì‹œ:

```python
import requests, pickle, base64
from PIL import Image

url = "http://localhost:8080/predict"

# ì´ë¯¸ì§€ ì¤€ë¹„ ë° ì¸ì½”ë”© (ì˜ˆì‹œ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”.)
img_paths = ["./img1.png", "./img2.png"]
pil_images = [Image.open(p).convert("RGB") for p in img_paths]
encoded_images_str = base64.b64encode(pickle.dumps(pil_images)).decode("utf-8")

example_payload = {
  "id": "test123",
  "images": encoded_images_str,
  "conversations": [
      {"role": "user", "content": " ì‚¬ì§„ ì†ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?"},
      {"role": "assistant", "content": "Thought: ì‚¬ì§„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.\n\nAction: ê°œê°€ ë³´ì…ë‹ˆë‹¤."}
  ]
}

response = requests.post(url, json=example_payload)
print(response.json())
```

---

## ğŸš© ì£¼ì˜ì‚¬í•­ ë° ì„¤ì¹˜ ë°©ë²• ìš”ì•½:

ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤:

```bash
pip install transformers torch torchvision fastapi uvicorn pillow accelerate einops sentencepiece flash-attn --upgrade
```

Flash Attention 2ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì¹˜í•˜ì„¸ìš”:

```bash
pip install -U flash-attn --no-build-isolation
```
