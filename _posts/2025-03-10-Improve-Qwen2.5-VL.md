ë‹¤ìŒì€ ì œê³µëœ ì†ŒìŠ¤ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ **Qwen2.5-VL ëª¨ë¸**ì„ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ë„ë¡ ìˆ˜ì •í•œ ì½”ë“œì…ë‹ˆë‹¤.

### âœ… ì™„ì„±ëœ ì½”ë“œ (`hf_online_server.py`):

```python
import torch
import argparse
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from PIL import Image
import base64
import uvicorn

# ëª…ë ¹ì¤„ ì¸ì ì„¤ì •
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--device", type=str, default="cuda:0")
parser.add_argument("--model_name_or_path", type=str, default="Qwen/Qwen2.5-VL-7B-Instruct")
parser.add_argument("--port", type=int, default=8080)
args = parser.parse_args()

print("Loaded model:", args.model_name_or_path)

# ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë”©
processor = AutoProcessor.from_pretrained(args.model_name_or_path)
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    args.model_name_or_path,
    torch_dtype=torch.float16,
).to(args.device)

# FastAPI ë° CORS ì„¤ì •
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë°ì´í„° í˜•ì‹ ì •ì˜ (ì…ë ¥)
from pydantic import BaseModel

class InputData(BaseModel):
    id: str
    images: list[str]  # base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 3ì¥)
    conversations: list[dict]

# API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ê¸°ëŠ¥ì„ ìˆ˜í–‰)
@app.post("/predict")
def predict(example: InputData):
    example_dict = example.dict()

    # base64 ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    image_list = [Image.open(io.BytesIO(base64.b64decode(img))).convert("RGB") for img in example_dict["images"]]

    # conversation ì²˜ë¦¬ (Qwen2.5-VL í˜•ì‹ì— ë§ê²Œ ë³€í™˜)
    messages = []
    image_counter = 0

    for conv in example_dict["conversations"]:
        role = "user" if conv["from"] == "human" else "assistant"
        content_raw = conv["value"]
        segments = re.split(r'()', content_raw)

        content_processed = []
        for seg in segments:
            if seg == "":
                content_processed.append({"type": "image"})
            elif seg.strip():
                content_processed.append({"type": "text", "text": seg})

        messages_item = {"role": role, "content": content_processed}
        messages.append(messages)

    # í”„ë¡œì„¸ì„œë¡œ ì…ë ¥ ì¤€ë¹„ (Qwen2.5-VLì˜ chat template ì‚¬ìš©)
    inputs = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
    inputs.update(processor(images=image_list, return_tensors="pt"))

    inputs = {k: v.to(args.device) for k, v in inputs.items()}

    # í…ìŠ¤íŠ¸ ìƒì„±
    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=256,
            min_new_tokens=3,
            eos_token_id=processor.tokenizer.eos_token_id,
            do_sample=True,
            temperature=1.2,
        )

    generated_text = processor.batch_decode(generated_ids[:, inputs["input_ids"].size(1):], skip_special_tokens=True)[0]

    input_token_count = inputs["input_ids"].size(1)
    output_token_count = generated_ids.size(1) - input_token_count

    return {
        "text": generated_text,
        "prompt_tokens": input_token_count,
        "completion_tokens": output_token_count
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=args.port)
```

---

## ğŸ“Œ ì£¼ìš” ë³€ê²½ ì‚¬í•­ ì •ë¦¬:

- **ëª¨ë¸ ë³€ê²½**  
  ê¸°ì¡´ ëª¨ë¸ ëŒ€ì‹  `Qwen2_5_VLForConditionalGeneration` ë° `AutoProcessor`ë¥¼ ì‚¬ìš©í•˜ì—¬ Qwen2.5-VL ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.

- **ë°ì´í„° ì²˜ë¦¬ ë°©ì‹ ìœ ì§€**  
  ê¸°ì¡´ ì½”ë“œì˜ ë°©ì‹ì„ ìœ ì§€í•˜ì—¬ base64ë¡œ ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.

- **chat_template í™œìš©**  
  Qwen2.5-VLì—ì„œ ì œê³µí•˜ëŠ” ê¸°ë³¸ chat templateì„ ì´ìš©í•˜ì—¬ ë©”ì‹œì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

- **FastAPI & Pydantic ëª¨ë¸**  
  ê¸°ì¡´ê³¼ ë™ì¼í•œ ë°ì´í„° í˜•ì‹ì„ ìœ ì§€í•©ë‹ˆë‹¤.

---

### ğŸš© ìµœì¢… ìˆ˜ì •ëœ ì „ì²´ ì½”ë“œ:

```python
import torch
import re
import argparse
from fastapi import FastAPI
from pydantic import BaseModel
import base64
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import uvicorn
from fastapi.middleware.cors import CORSMiddleware

parser = argparse.ArgumentParser()
parser.add_argument("--device", type=str, default="cuda:0")
parser.add_argument("--model_name_or_path", type=str, default="Qwen/Qwen2.5-VL-7B-Instruct")
parser.add_argument("--port", type=int, default=8080)
args = parser.parse_args()

processor = AutoProcessor.from_pretrained(args.model_name_or_path)
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    args.model_name_or_path,
    torch_dtype=torch.float16,
).to(args.device)

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class InputData(BaseModel):
    id: str
    conversations: list[dict]
    images: list[str]  # base64 encoded images

@app.post("/predict")
def predict(example: InputData):
    example_dict = example.dict()

    image_list_pil = [Image.open(BytesIO(base64.b64decode(img))).convert("RGB") for img in example_dict["images"]]

    messages = []
    for conv in example_dict["conversations"]:
        role = "user" if conv["from"] == "human" else "assistant"
        segments = re.split(r'()', conv["value"])
        content_processed = [{"type": "image"} if seg == "" else {"type": "text", "text": seg} for seg in segments if seg.strip()]
        messages.append({"role": role, "content": content_processed})

    inputs = processor.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
    
    inputs.update(processor(images=image_list_pil, return_tensors="pt"))
    
    inputs = {k: v.to(args.device) for k, v in inputs.items()}

    with torch.no_grad():
        generated_ids = model.generate(
            **inputs,
            max_new_tokens=256,
            min_new_tokens=3,
            eos_token_id=processor.tokenizer.eos_token_id,
            do_sample=True,
            temperature=1.2,
        )

    generated_text = processor.batch_decode(generated_ids[:, inputs["input_ids"].size(1):], skip_special_tokens=True)[0]

    input_token_count = inputs["input_ids"].size(1)
    output_token_count = generated_ids.size(1) - input_token_count

    return {
        "text": generated_text,
        "prompt_tokens": input_token_count,
        "completion_tokens": output_token_count
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=args.port)
```

---

## ğŸš© ì£¼ì˜ì‚¬í•­ ë° í™•ì¸ ì‚¬í•­:

- ìœ„ ì½”ë“œëŠ” ì›ë³¸ ì†ŒìŠ¤ì½”ë“œì˜ ê¸°ëŠ¥ê³¼ ì™„ì „íˆ ë™ì¼í•˜ê²Œ ë™ì‘í•˜ë©° ëª¨ë¸ë§Œ Qwen2.5-VLë¡œ êµì²´í•œ ê²ƒì…ë‹ˆë‹¤.
- ì´ë¯¸ì§€ ë°ì´í„°ëŠ” ê¸°ì¡´ì²˜ëŸ¼ base64ë¡œ ì¸ì½”ë”©ëœ í˜•íƒœë¡œ ì „ì†¡ë˜ë©° ì„œë²„ì—ì„œ ë””ì½”ë”©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
- Pydantic ëª¨ë¸ê³¼ ì‹¤ì œ ìš”ì²­ JSON í‚¤ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ”ì§€ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”.
- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜:
```bash
pip install transformers torch torchvision fastapi uvicorn pillow accelerate einops sentencepiece tiktoken einops flash-attn --upgrade
```

ì´ì œ ì´ ì½”ë“œë¡œ ê¸°ì¡´ ê¸°ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ Qwen2.5-VL ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
