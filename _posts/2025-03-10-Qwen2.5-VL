기존 코드를 Qwen2.5-VL 모델을 사용하는 방식으로 수정하려면, 모델 로딩 부분과 프로세서 설정 부분을 수정해야 합니다. Qwen2.5-VL 모델은 `Qwen2_5_VLForConditionalGeneration` 클래스를 사용해야 하며, 프로세서도 `AutoProcessor`를 통해 적절한 모델의 프로세서를 불러와야 합니다. 아래는 수정된 코드입니다.

```python
import os
import re
import json
import torch
import pickle
import argparse
from fastapi import FastAPI
from pydantic import BaseModel
import base64
from PIL import Image
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
import uvicorn
from fastapi.middleware.cors import CORSMiddleware

# argparse 설정
parser = argparse.ArgumentParser()
parser.add_argument("--device", type=str, default="cuda:0")
parser.add_argument("--model_name_or_path", type=str,
                    default="Qwen/Qwen2.5-VL-7B-Instruct")
parser.add_argument("--port", type=int, default=8080)
args = parser.parse_args()
print(args)

print("Current loaded model:", args.model_name_or_path)

# 프로세서 설정
processor = AutoProcessor.from_pretrained(
    args.model_name_or_path,
    do_image_splitting=False
)
processor.image_processor.size['longest_edge'] = 980
processor.image_processor.size['shortest_edge'] = 980

class MyDataCollator:
    def __init__(self, processor):
        self.processor = processor
        # Qwen2.5-VL 모델에서는 특수 토큰을 다르게 처리할 수 있으므로, 필요에 따라 수정
        self.image_token_id = None  # Qwen2.5-VL에서 특수 토큰을 사용하지 않을 경우 None으로 설정

    def __call__(self, example_list):
        texts = []
        images = []
        for example in example_list:
            image_list = example["images"]
            
            messages = []
            conversations = example["conversations"]
            for conv in conversations:
                item = {}
                item["role"] = conv["role"]
                raw_content = conv["content"]
                raw_content_split = re.split(r'()', raw_content)
                content_list = [{"type": "image"} if seg == ""
                            else {"type": "text", "text": seg} for seg in raw_content_split]
                item["content"] = content_list
                messages.append(item)
        

            text = processor.apply_chat_template(messages, add_generation_prompt=True)
            texts.append(text.strip())
            images.append(image_list)

        batch = processor(text=texts, images=images, return_tensors="pt", padding=True)

        return batch

# 모델 로딩
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    args.model_name_or_path,
    torch_dtype=torch.float16,
).to(args.device)

data_collator = MyDataCollator(processor)

class InputData(BaseModel):
    id: str
    conversations: list
    images: str

class OutputPrediction(BaseModel):
    generated_text: str


app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/predict")
def predict(example: InputData):
    example = example.dict()
    image_list_bin = base64.b64decode(example["images"])
    image_list = pickle.loads(image_list_bin)
    example["images"] = image_list
    batch = data_collator([example])
    batch = {k: v.to(args.device) for k, v in batch.items()}
    with torch.no_grad():
        generated_ids = model.generate(**batch, max_new_tokens=256, min_new_tokens=3, 
                                eos_token_id=processor.tokenizer.eos_token_id, do_sample=True, temperature=1.2)
    generated_text = processor.batch_decode(generated_ids[:, batch["input_ids"].size(1):], skip_special_tokens=True)
    generated_text = generated_text[0]
    
    input_token_count = batch["input_ids"].size(1)
    output_token_count = generated_ids.size(1) - input_token_count
    return {"text": generated_text, "prompt_tokens": input_token_count, "completion_tokens": output_token_count}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=args.port)
```

### 주의사항:
1. **모델 이름 및 경로**: `args.model_name_or_path`는 `"Qwen/Qwen2.5-VL-7B-Instruct"`와 같은 형식으로 설정되어야 합니다.
2. **특수 토큰 처리**: Qwen2.5-VL 모델에서 특수 토큰을 사용하지 않을 경우, `self.image_token_id`를 `None`으로 설정합니다.
3. **EOS 토큰 ID**: `eos_token_id`는 `processor.tokenizer.eos_token_id`로 설정합니다. 

이 코드는 기존 기능을 유지하면서 Qwen2.5-VL 모델을 사용하도록 수정되었습니다.
