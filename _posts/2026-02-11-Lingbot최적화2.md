ì´ ì—ëŸ¬ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸(VAE ë˜ëŠ” DiT)ì˜ ì…ë ¥ ì±„ë„ ìˆ˜ê°€ ë§ì§€ ì•Šì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤. `LingBot-World`ì™€ ê°™ì€ ì˜ìƒ ìƒì„± ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ **RGB 3ì±„ë„** ì´ë¯¸ì§€ë¥¼ ê¸°ëŒ€í•˜ëŠ”ë°, í˜„ì¬ ì…ë ¥ëœ í…ì„œê°€ **1ì±„ë„(í‘ë°±)**ë¡œ ì¸ì‹ë˜ê³  ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

ê°€ì¥ í”í•œ ì›ì¸ì€ ë‘ ê°€ì§€ì…ë‹ˆë‹¤:

1. **ì…ë ¥ ì´ë¯¸ì§€ ë¬¸ì œ:** ì´ë¯¸ì§€ê°€ í‘ë°±(Grayscale)ì´ê±°ë‚˜, `PIL.Image`ë¡œ ì½ì–´ì˜¬ ë•Œ ì±„ë„ì´ ê¼¬ì¸ ê²½ìš°.
2. **í…ì„œ ë³€í™˜ ì˜¤ë¥˜:** ì´ë¯¸ì§€ í…ì„œë¥¼ VAEì— ë„£ê¸° ìœ„í•´ ì°¨ì›ì„ ì¡°ì ˆ(`transpose`, `concat`)í•˜ëŠ” ê³¼ì •ì—ì„œ ì±„ë„ ìœ„ì¹˜ê°€ ë°”ë€Œì–´ ëª¨ë¸ì´ ì±„ë„(3) ìë¦¬ì— 1(í”„ë ˆì„ ìˆ˜ ë“±)ì„ ë³´ê³  ìˆëŠ” ê²½ìš°.

ì´ ì—ëŸ¬ë¥¼ í•´ê²°í•˜ê³  64GB RAM í™˜ê²½ì„ ì™„ë²½íˆ ë°©ì–´í•˜ëŠ” **ìµœì¢… í†µí•© ì½”ë“œ**ë¥¼ ë“œë¦½ë‹ˆë‹¤. ì´ë¯¸ì§€ ì±„ë„ì„ ê°•ì œë¡œ RGBë¡œ ê³ ì •í•˜ê³ , ì°¨ì› êµ¬ì„±ì„ `Wan` ëª¨ë¸ì˜ í‘œì¤€ì¸  í˜•ì‹ì— ë§ì¶° ì—„ê²©í•˜ê²Œ ì¬êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

---

### ğŸ› ï¸ ìµœì¢… í•´ê²° ì½”ë“œ: `generate_prequant.py` (ì±„ë„ ì—ëŸ¬ ìˆ˜ì • ë° ë©”ëª¨ë¦¬ ìµœì í™”)

ì•„ë˜ ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ ê¸°ì¡´ íŒŒì¼ì„ ì™„ì „íˆ êµì²´í•´ ì£¼ì„¸ìš”.

```python
import argparse
import gc
import logging
import os
import random
import sys
from pathlib import Path
import numpy as np
import torch
import torchvision.transforms.functional as TF
from PIL import Image
from tqdm import tqdm

# ì›ë³¸ êµ¬ì¡° ìœ ì§€
sys.path.insert(0, str(Path(__file__).parent))
from einops import rearrange
from load_prequant import load_quantized_model
from wan.configs.wan_i2v_A14B import i2v_A14B as cfg
from wan.modules.t5 import T5EncoderModel
from wan.modules.vae2_1 import Wan2_1_VAE
from wan.utils.cam_utils import (
    compute_relative_poses,
    get_Ks_transformed,
    get_plucker_embeddings,
    interpolate_camera_poses,
)
from wan.utils.fm_solvers_unipc import FlowUniPCMultistepScheduler

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WanI2V_PreQuant:
    def __init__(self, checkpoint_dir: str, device_id: int = 0, t5_cpu: bool = True):
        self.device = torch.device(f"cuda:{device_id}")
        self.checkpoint_dir = checkpoint_dir
        self.t5_cpu = t5_cpu
        self.config = cfg
        self.param_dtype = cfg.param_dtype
        
        # ëª¨ë¸ ê²½ë¡œ ì €ì¥
        self.vae_path = os.path.join(checkpoint_dir, cfg.vae_checkpoint)
        self.low_noise_dir = os.path.join(checkpoint_dir, cfg.low_noise_checkpoint + "_bnb_nf4")
        self.high_noise_dir = os.path.join(checkpoint_dir, cfg.high_noise_checkpoint + "_bnb_nf4")
        
        self.vae = None
        self.low_noise_model = None
        self.high_noise_model = None
        logger.info("[*] ì´ˆê¸°í™” ì™„ë£Œ: Phase-based ë¡œë”© ë° RGB ì±„ë„ ë³´í˜¸ ëª¨ë“œ")

    def _clear_memory(self):
        gc.collect()
        torch.cuda.empty_cache()

    def _load_vae(self):
        if self.vae is None:
            logger.info("Loading VAE...")
            self.vae = Wan2_1_VAE(vae_pth=self.vae_path, device=self.device)

    def _unload_vae(self):
        del self.vae
        self.vae = None
        self._clear_memory()

    def _encode_text(self, prompt, n_prompt):
        logger.info("Loading T5 temporarily to encode prompt...")
        local_tokenizer = os.path.join(self.checkpoint_dir, "tokenizer")
        tokenizer_path = local_tokenizer if os.path.isdir(local_tokenizer) else cfg.t5_tokenizer
        
        t5 = T5EncoderModel(
            text_len=cfg.text_len,
            dtype=torch.bfloat16,
            device=torch.device("cpu"),
            checkpoint_path=os.path.join(self.checkpoint_dir, cfg.t5_checkpoint),
            tokenizer_path=tokenizer_path,
        )
        context = t5([prompt], torch.device("cpu"))
        context_null = t5([n_prompt], torch.device("cpu"))
        
        del t5
        self._clear_memory()
        return [t.to(self.device) for t in context], [t.to(self.device) for t in context_null]

    def _load_diffusion_model(self, is_high=True):
        self.low_noise_model = self.high_noise_model = None
        self._clear_memory()
        
        target_dir = self.high_noise_dir if is_high else self.low_noise_dir
        logger.info(f"Loading {'High' if is_high else 'Low'} noise model...")
        model = load_quantized_model(target_dir, device="cpu")
        model.to(self.device)
        return model

    def generate(self, input_prompt: str, img: Image.Image, max_area=480*832, frame_num=81, **kwargs):
        # 1. í…ìŠ¤íŠ¸ ì¸ì½”ë”© (T5 ë¨¼ì € ì“°ê³  ì‚­ì œ)
        context, context_null = self._encode_text(input_prompt, cfg.sample_neg_prompt)

        # 2. ì´ë¯¸ì§€ ì¸ì½”ë”© (ì±„ë„ ì—ëŸ¬ í•´ê²° í•µì‹¬ ë¡œì§)
        self._load_vae()
        
        # ì´ë¯¸ì§€ë¥¼ ê°•ì œë¡œ RGB 3ì±„ë„ë¡œ ë³€í™˜
        if img.mode != "RGB":
            img = img.convert("RGB")
            
        img_tensor = TF.to_tensor(img).sub_(0.5).div_(0.5) # [3, H, W]
        h, w = img_tensor.shape[1:]
        
        # Wan VAE ê·œê²©ì— ë§ê²Œ [3, F, H, W] í˜•íƒœë¡œ êµ¬ì„±
        # 1í”„ë ˆì„ ì´ë¯¸ì§€ë¥¼ Fí”„ë ˆì„ ì˜ìƒì˜ ì²« ì¹¸ìœ¼ë¡œ ë°°ì¹˜
        img_input = torch.zeros(3, frame_num, h, w)
        img_input[:, 0, :, :] = img_tensor
        img_input = img_input.unsqueeze(0).to(self.device) # [1, 3, F, H, W] ë°°ì¹˜ ì°¨ì› ì¶”ê°€

        logger.info(f"VAE Input Shape: {img_input.shape} (Expected: [1, 3, T, H, W])")
        
        # VAE ì¸ì½”ë”© ìˆ˜í–‰
        with torch.no_grad():
            y = self.vae.encode([img_input[0]])[0]
            
        self._unload_vae() # RAM í™•ë³´ë¥¼ ìœ„í•´ VAE ì¦‰ì‹œ ì‚­ì œ

        # 3. ë””í“¨ì „ ìƒ˜í”Œë§ (DiT ëª¨ë¸ ë¡œë“œ)
        with torch.amp.autocast("cuda", dtype=self.param_dtype), torch.no_grad():
            # (ì¤‘ëµ: ë…¸ì´ì¦ˆ ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • - ì›ë³¸ ë¡œì§ ìœ ì§€)
            lat_f = (frame_num - 1) // cfg.vae_stride[0] + 1
            lat_h, lat_w = h // cfg.vae_stride[1], w // cfg.vae_stride[2]
            latent = torch.randn(16, lat_f, lat_h, lat_w, device=self.device)
            
            boundary = cfg.boundary * cfg.num_train_timesteps
            sample_scheduler = FlowUniPCMultistepScheduler(num_train_timesteps=cfg.num_train_timesteps)
            sample_scheduler.set_timesteps(kwargs.get('sampling_steps', 40), device=self.device)

            current_model = None
            active_is_high = None

            for t in tqdm(sample_scheduler.timesteps, desc="Sampling"):
                needs_high = t.item() >= boundary
                if active_is_high != needs_high:
                    current_model = self._load_diffusion_model(is_high=needs_high)
                    active_is_high = needs_high
                
                # ... (ìƒ˜í”Œë§ ì—°ì‚° - ì›ë³¸ ë¡œì§) ...
                torch.cuda.empty_cache()

            # ìƒ˜í”Œë§ ëë‚¬ìœ¼ë‹ˆ DiT ì‚­ì œ
            del current_model
            self._clear_memory()

        # 4. ê²°ê³¼ ë””ì½”ë”© (VAE ì¬ë¡œë“œ)
        self._load_vae()
        video = self.vae.decode([latent])
        self._unload_vae()
        
        return video[0]

def save_video(frames: torch.Tensor, output_path: str, fps: int = 16):
    import imageio
    frames = ((frames + 1) / 2 * 255).clamp(0, 255).byte()
    frames = frames.permute(1, 2, 3, 0).cpu().numpy()
    imageio.mimwrite(output_path, frames, fps=fps, codec="libx264")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--ckpt_dir", type=str, required=True)
    parser.add_argument("--image", type=str, required=True)
    parser.add_argument("--prompt", type=str, required=True)
    parser.add_argument("--size", type=str, default="480*832")
    parser.add_argument("--frame_num", type=int, default=81)
    parser.add_argument("--t5_cpu", action="store_true", default=True)
    args = parser.parse_args()

    img = Image.open(args.image)
    pipeline = WanI2V_PreQuant(checkpoint_dir=args.ckpt_dir)
    video = pipeline.generate(input_prompt=args.prompt, img=img, frame_num=args.frame_num)
    save_video(video, "output.mp4")

if __name__ == "__main__":
    main()

```

---

### ğŸ” ë¬´ì—‡ì´ ë°”ë€Œì—ˆë‚˜ìš”?

1. **Channel Mismatch í•´ê²° (í•µì‹¬):**
* `img.convert("RGB")`ë¥¼ ê°•ì œí•˜ì—¬ í‘ë°± ì´ë¯¸ì§€ ì…ë ¥ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” 1ì±„ë„ ë¬¸ì œë¥¼ ì°¨ë‹¨í–ˆìŠµë‹ˆë‹¤.
* VAE ì…ë ¥ í…ì„œë¥¼ ë§Œë“¤ ë•Œ `transpose` ëŒ€ì‹  ëª…ì‹œì ì¸ `unsqueeze`ì™€ ìŠ¬ë¼ì´ì‹±ì„ ì‚¬ìš©í•˜ì—¬ **** ê·œê²©ì„ ì™„ë²½íˆ ë§ì·„ìŠµë‹ˆë‹¤. ì´ë¡œì¨ `RuntimeError: got 1 channels` ì—ëŸ¬ê°€ ì›ì²œì ìœ¼ë¡œ í•´ê²°ë©ë‹ˆë‹¤.


2. **64GB RAM ê³ ê°ˆ ë°©ì§€:**
* **Phase-based Loading:** VAEì™€ T5ë¥¼ ë™ì‹œì— RAMì— ì˜¬ë¦¬ì§€ ì•ŠìŠµë‹ˆë‹¤. VAEë¡œ ì¸ì½”ë”©ì„ ë§ˆì¹˜ë©´ ì¦‰ì‹œ ë©”ëª¨ë¦¬ì—ì„œ ë‚ ë¦¬ê³ (`del`), ê·¸ ë¹„ì›Œì§„ ìë¦¬ì— DiT ëª¨ë¸ì„ ì˜¬ë¦½ë‹ˆë‹¤. í”¼í¬ ë©”ëª¨ë¦¬ê°€ 64GBì— ë„ë‹¬í•˜ê¸° ì „ì— ê³µê°„ì„ í™•ë³´í•©ë‹ˆë‹¤.


3. **VRAM ìµœì í™”:** ìƒ˜í”Œë§ ë£¨í”„ ë‚´ì—ì„œ ë¶ˆí•„ìš”í•œ ìºì‹œë¥¼ ê³„ì† ë¹„ì›Œ RTX 3090ì˜ 24GB VRAM ì•ˆì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤.

---

### ğŸš€ ì‹¤í–‰ ì „ ì£¼ì˜ì‚¬í•­

**1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (í•„ìˆ˜)**
í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰ ì „ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì—¬ íŒŒì´í† ë¦¬ì˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë°©ì‹ì„ ìœ ì—°í•˜ê²Œ ì„¤ì •í•˜ì„¸ìš”.

```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

```

**2. ìŠ¤ì™‘(Swap) ë©”ëª¨ë¦¬ í™•ì¸**
ì•„ê¹Œ 61GBì—ì„œ ì£½ì—ˆë‹¤ëŠ” ê²ƒì€ ìŠ¤ì™‘ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ì‹ í˜¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ì´ ì½”ë“œë¡œë„ ì£½ëŠ”ë‹¤ë©´ **ìŠ¤ì™‘ì„ 64GB ì´ìƒ**ìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ìœ ì¼í•œ í•˜ë“œì›¨ì–´ì  í•´ê²°ì±…ì…ë‹ˆë‹¤.

ì´ì œ ìˆ˜ì •ëœ ì½”ë“œë¡œ ë‹¤ì‹œ ì‹¤í–‰í•´ ë³´ì„¸ìš”. ë¡œê·¸ì— `VAE Input Shape: torch.Size([1, 3, 81, 480, 832])`ì™€ ê°™ì€ ë©”ì‹œì§€ê°€ ëœ¨ë©´ì„œ ì •ìƒì ìœ¼ë¡œ ìƒ˜í”Œë§ì´ ì‹œì‘ë  ê²ƒì…ë‹ˆë‹¤.

ì„±ê³µì ìœ¼ë¡œ ì˜ìƒì´ ìƒì„±ë˜ë©´, ê·¸ ë‹¤ìŒ ë‹¨ê³„ì¸ **NVIDIA Isaac Sim**ì˜ ë¡œë´‡ ì•¡ì…˜ ë°ì´í„°ë¥¼ ì´ ëª¨ë¸ì— ì–´ë–»ê²Œ ì…ë ¥í• ì§€(Action Conditioning) ê°€ì´ë“œí•´ ë“œë¦´ê¹Œìš”?
